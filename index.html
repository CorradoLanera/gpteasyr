<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A basic and simple interface to OpenAI’s GPT API • gpteasyr</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="A basic and simple interface to OpenAI’s GPT API">
<meta name="description" content="The goal of gpteasyr is to provide a basic and simple interface to OpenAI's GPT API (and other compatible APIs). The package is also designed to work with (i.e., to query on) dataframes/tibbles, and to simplify the process of querying the API.">
<meta property="og:description" content="The goal of gpteasyr is to provide a basic and simple interface to OpenAI's GPT API (and other compatible APIs). The package is also designed to work with (i.e., to query on) dataframes/tibbles, and to simplify the process of querying the API.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">gpteasyr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.5.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/CorradoLanera/gpteasyr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="gpteasyr">gpteasyr<a class="anchor" aria-label="anchor" href="#gpteasyr"></a>
</h1></div>
<!-- badges: start -->

<p>The goal of <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a> is to provide a basic/simple interface to OpenAI’s GPT API. The package is designed to work with dataframes/tibbles and to simplify the process of querying the API.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install the development version of <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a> like so:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">remotes</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"CorradoLanera/gpteasyr"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="basic-example">Basic example<a class="anchor" aria-label="anchor" href="#basic-example"></a>
</h2>
<p>You can use the <code>query_gpt</code> function to query the GPT API. You can decide the model to use (e.g., <code>gpt-3.5-turbo</code>, <code>gpt-4-turbo</code>, or <code>gpt-4o</code>). This function is useful because mainly it iterate the query a decided number of times (10 by default) in case of error (often caused by server overload).</p>
<p>To use the function, you need to compose a prompt. You can use (but it is not necessary!) the <code>compose_prompt_api</code> function to compose the prompt properly with an optional (single) system prompt (i.e., gpt’s setup) and a (single) user prompt (i.e., the query). This function is useful because it helps you to compose the prompt automatically adopting the required API’s structure.</p>
<blockquote>
<p>NOTE: you can still pass a multiple fully-formatted list (of lists) as described in the <a href="https://platform.openai.com/docs/api-reference/chat" class="external-link">official documentation</a> (<a href="https://platform.openai.com/docs/api-reference/chat" class="external-link uri">https://platform.openai.com/docs/api-reference/chat</a>).</p>
</blockquote>
<p>Once you have queried the API, you can extract the content of the response using the <code>get_content</code> function. You can also extract the tokens of the prompt and the response using the <code>get_tokens</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Wellcome to `{gpteasyr}`!</span></span>
<span><span class="co">#&gt; The OPENAI_API_KEY environment variable is set</span></span>
<span><span class="co">#&gt; You are ready to use the package `{gpteasyr}`.</span></span>
<span><span class="co">#&gt; Just, double check if the key is the correct one.</span></span>
<span><span class="co">#&gt; REMIND: Never share your API key with others.</span></span>
<span><span class="co">#&gt;       Keep it safe and secure.</span></span>
<span><span class="co">#&gt;       If you think that your API key was compromised,</span></span>
<span><span class="co">#&gt;       you can regenerate it in the OpenAI-API website</span></span>
<span><span class="co">#&gt;       (https://platform.openai.com/api-keys), or contacting your GPT's admin.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Enjoy GPT with `{gpteasyr}`!</span></span>
<span><span class="co">#&gt; If you like to use the Python backend</span></span>
<span><span class="co">#&gt;     (working for GPT's OpenAI requests only!),</span></span>
<span><span class="co">#&gt; setup its environment first by executing:</span></span>
<span><span class="co">#&gt; `setup_py()`</span></span>
<span><span class="co">#&gt;     (default virtual environment name is 'r-gpt-venv').</span></span>
<span><span class="co">#&gt; If you prefer to use a name different from the default one, run:</span></span>
<span><span class="co">#&gt; `setup_py("&lt;your_custom_environment_name&gt;")`</span></span>
<span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span></span>
<span>  sys_prompt <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  usr_prompt <span class="op">=</span> <span class="st">"Tell me about the last course you provided."</span></span>
<span><span class="op">)</span></span>
<span><span class="va">prompt</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; [[1]]$role</span></span>
<span><span class="co">#&gt; [1] "system"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[1]]$content</span></span>
<span><span class="co">#&gt; [1] "You are the assistant of a university professor."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; [[2]]$role</span></span>
<span><span class="co">#&gt; [1] "user"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]$content</span></span>
<span><span class="co">#&gt; [1] "Tell me about the last course you provided."</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>  prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>,</span>
<span>  quiet <span class="op">=</span> <span class="cn">FALSE</span>, <span class="co"># default TRUE</span></span>
<span>  max_try <span class="op">=</span> <span class="fl">2</span>, <span class="co"># default 10</span></span>
<span>  temperature <span class="op">=</span> <span class="fl">1.5</span>, <span class="co"># default 0 [0-2]</span></span>
<span>  max_tokens <span class="op">=</span> <span class="fl">100</span> <span class="co"># default the maximum allowed for the selected model</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; ℹ Total tries: 1.</span></span>
<span><span class="co">#&gt; ℹ Prompt token used: 29.</span></span>
<span><span class="co">#&gt; ℹ Response token used: 57.</span></span>
<span><span class="co">#&gt; ℹ Total token used: 86.</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 7</span></span>
<span><span class="co">#&gt;  $ id                : chr "chatcmpl-9mkWb4KXeBxHxJ0HF24gzI2wH35dD"</span></span>
<span><span class="co">#&gt;  $ object            : chr "chat.completion"</span></span>
<span><span class="co">#&gt;  $ created           : int 1721405613</span></span>
<span><span class="co">#&gt;  $ model             : chr "gpt-4o-mini-2024-07-18"</span></span>
<span><span class="co">#&gt;  $ choices           :'data.frame':  1 obs. of  4 variables:</span></span>
<span><span class="co">#&gt;   ..$ index        : int 0</span></span>
<span><span class="co">#&gt;   ..$ message      :'data.frame':    1 obs. of  2 variables:</span></span>
<span><span class="co">#&gt;   .. ..$ role   : chr "assistant"</span></span>
<span><span class="co">#&gt;   .. ..$ content: chr "I'm unable to provide specifics about a particular course, as I don't have access to personal coursework or tea"| __truncated__</span></span>
<span><span class="co">#&gt;   ..$ logprobs     : logi NA</span></span>
<span><span class="co">#&gt;   ..$ finish_reason: chr "stop"</span></span>
<span><span class="co">#&gt;  $ usage             :List of 3</span></span>
<span><span class="co">#&gt;   ..$ prompt_tokens    : int 29</span></span>
<span><span class="co">#&gt;   ..$ completion_tokens: int 57</span></span>
<span><span class="co">#&gt;   ..$ total_tokens     : int 86</span></span>
<span><span class="co">#&gt;  $ system_fingerprint: chr "fp_7dd529cfca"</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "I'm unable to provide specifics about a particular course, as I don't have access to personal coursework or teaching records. However, I can certainly assist you in general terms about course content, syllabus structure, projects, or methodologies! Please let me know which subject or specific information you're interested in."</span></span>
<span></span>
<span><span class="co"># for a well formatted output on R, use `cat()`</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; I'm unable to provide specifics about a particular course, as I don't have access to personal coursework or teaching records. However, I can certainly assist you in general terms about course content, syllabus structure, projects, or methodologies! Please let me know which subject or specific information you're interested in.</span></span>
<span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span> <span class="co"># default is "total"</span></span>
<span><span class="co">#&gt; [1] 86</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span>, <span class="st">"prompt"</span><span class="op">)</span> <span class="co"># "total", "prompt", "completion" (i.e., the answer)</span></span>
<span><span class="co">#&gt; [1] 29</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span>, <span class="st">"all"</span><span class="op">)</span></span>
<span><span class="co">#&gt;     prompt_tokens completion_tokens      total_tokens </span></span>
<span><span class="co">#&gt;                29                57                86</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="easy-prompt-assisted-creation">Easy prompt-assisted creation<a class="anchor" aria-label="anchor" href="#easy-prompt-assisted-creation"></a>
</h2>
<p>You can use the <code>compose_sys_prompt</code> and <code>compose_usr_prompt</code> functions to create the system and user prompts, respectively. These functions are useful because they help you to compose the prompts following best practices in composing prompt. In fact the arguments are just the main components every good prompt should have. They do just that, composing the prompt for you juxtaposing the components in order.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sys_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_sys_prompt</a></span><span class="op">(</span></span>
<span>  role <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  context <span class="op">=</span> <span class="st">"You are analyzing the comments of the students of the last course."</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">sys_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; You are the assistant of a university professor.</span></span>
<span><span class="co">#&gt; You are analyzing the comments of the students of the last course.</span></span>
<span></span>
<span><span class="va">usr_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_usr_prompt</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="st">"Your task is to extract information from a text provided."</span>,</span>
<span>  instructions <span class="op">=</span> <span class="st">"You should extract the first and last words of the text."</span>,</span>
<span>  output <span class="op">=</span> <span class="st">"Return the first and last words of the text separated by a dash, i.e., `first - last`."</span>,</span>
<span>  style <span class="op">=</span> <span class="st">"Do not add any additional information, return only the requested information."</span>,</span>
<span>  examples <span class="op">=</span> <span class="st">"</span></span>
<span><span class="st">    # Examples:</span></span>
<span><span class="st">    text: 'This is an example text.'</span></span>
<span><span class="st">    output: 'This - text'</span></span>
<span><span class="st">    text: 'Another example text!!!'</span></span>
<span><span class="st">    output: 'Another - text'"</span>,</span>
<span>  text <span class="op">=</span> <span class="st">"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura"</span>,</span>
<span>  closing <span class="op">=</span> <span class="st">"Take a deep breath and work on the problem step-by-step."</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">usr_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; Your task is to extract information from a text provided.</span></span>
<span><span class="co">#&gt; You should extract the first and last words of the text.</span></span>
<span><span class="co">#&gt; Return the first and last words of the text separated by a dash, i.e., `first - last`.</span></span>
<span><span class="co">#&gt; Do not add any additional information, return only the requested information.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     # Examples:</span></span>
<span><span class="co">#&gt;     text: 'This is an example text.'</span></span>
<span><span class="co">#&gt;     output: 'This - text'</span></span>
<span><span class="co">#&gt;     text: 'Another example text!!!'</span></span>
<span><span class="co">#&gt;     output: 'Another - text'</span></span>
<span><span class="co">#&gt; """</span></span>
<span><span class="co">#&gt; Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura</span></span>
<span><span class="co">#&gt; """</span></span>
<span><span class="co">#&gt; Take a deep breath and work on the problem step-by-step.</span></span>
<span></span>
<span><span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span><span class="va">sys_prompt</span>, <span class="va">usr_prompt</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Nel - oscura"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="querying-a-column-of-a-dataframe">Querying a column of a dataframe<a class="anchor" aria-label="anchor" href="#querying-a-column-of-a-dataframe"></a>
</h2>
<p>You can use the <code>query_gpt_on_column</code> function to query the GPT API on a column of a dataframe. This function is useful because it helps you to iterate the query on each row of the column and to compose the prompt automatically adopting the required API’s structure. In this case, you need to provide the components of the prompt creating the prompt template, and the name of the column you what to embed in the template as a “text” to query. All the prompt’s components are optional, so you can provide only the ones you need: <code>role</code> and <code>context</code> compose the system prompt, while <code>task</code>, <code>instructions</code>, <code>output</code>, <code>style</code>, and <code>examples</code> compose the user prompt (they will be just juxtaposed in the right order)</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">db</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  txt <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"I'm very satisfied with the course; it was very interesting and useful."</span>,</span>
<span>    <span class="st">"I didn't like it at all; it was deadly boring."</span>,</span>
<span>    <span class="st">"The best course I've ever attended."</span>,</span>
<span>    <span class="st">"The course was a waste of time."</span>,</span>
<span>    <span class="st">"blah blah blah"</span>,</span>
<span>    <span class="st">"woow"</span>,</span>
<span>    <span class="st">"bim bum bam"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># system</span></span>
<span><span class="va">role</span> <span class="op">&lt;-</span> <span class="st">"You are the assistant of a university professor."</span></span>
<span><span class="va">context</span> <span class="op">&lt;-</span> <span class="st">"You are analyzing the comments of the students of the last course."</span></span>
<span></span>
<span><span class="co"># user</span></span>
<span><span class="va">task</span> <span class="op">&lt;-</span> <span class="st">"Your task is to understand if they are satisfied with the course."</span></span>
<span><span class="va">instructions</span> <span class="op">&lt;-</span> <span class="st">"Analyze the comments and decide if they are satisfied or not."</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="st">"Report 'satisfied' or 'unsatisfied', in case of doubt or impossibility report 'NA'."</span></span>
<span><span class="va">style</span> <span class="op">&lt;-</span> <span class="st">"Do not add any comment, return only and exclusively one of the possible classifications."</span></span>
<span></span>
<span><span class="va">examples</span> <span class="op">&lt;-</span> <span class="st">"</span></span>
<span><span class="st">  # Examples:</span></span>
<span><span class="st">  text: 'I'm very satisfied with the course; it was very interesting and useful.'</span></span>
<span><span class="st">  output: 'satisfied'</span></span>
<span><span class="st">  text: 'I didn't like it at all; it was deadly boring.'</span></span>
<span><span class="st">  output: 'unsatisfied'"</span></span>
<span></span>
<span><span class="va">closing</span> <span class="op">&lt;-</span> <span class="st">"Take a deep breath and work on the problem step-by-step."</span> <span class="co"># This will be added AFTER the embedded text</span></span>
<span></span>
<span><span class="va">sys_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_sys_prompt</a></span><span class="op">(</span>role <span class="op">=</span> <span class="va">role</span>, context <span class="op">=</span> <span class="va">context</span><span class="op">)</span></span>
<span><span class="va">usr_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_usr_prompt</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="va">task</span>,</span>
<span>  instructions <span class="op">=</span> <span class="va">instructions</span>,</span>
<span>  output <span class="op">=</span> <span class="va">output</span>,</span>
<span>  style <span class="op">=</span> <span class="va">style</span>,</span>
<span>  examples <span class="op">=</span> <span class="va">examples</span></span>
<span>  <span class="co"># If you want to put a `closing` after the text embedded by the use of</span></span>
<span>  <span class="co"># `query_gpt_on_column`, you usually shouldn't include it here as</span></span>
<span>  <span class="co"># well: if put here, it will go after the examples but before the text</span></span>
<span>  <span class="co"># embedded by `query_gpt_on_column`; In borderline cases, you might</span></span>
<span>  <span class="co"># still free to decide to put it here, or even both.</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">db</span> <span class="op">|&gt;</span></span>
<span> <span class="fu"><a href="reference/query_gpt_on_column.html">query_gpt_on_column</a></span><span class="op">(</span></span>
<span>   text_column <span class="op">=</span> <span class="st">"txt"</span>,  <span class="co"># the name of the column containing the text to</span></span>
<span>                         <span class="co"># analyze after being embedded in the prompt.</span></span>
<span>   sys_prompt <span class="op">=</span> <span class="va">sys_prompt</span>,</span>
<span>   usr_prompt <span class="op">=</span> <span class="va">usr_prompt</span>,</span>
<span>   closing <span class="op">=</span> <span class="va">closing</span>,  <span class="co"># this will be added AFTER the embedded text</span></span>
<span>   na_if_error <span class="op">=</span> <span class="cn">TRUE</span>,  <span class="co"># dafault is FALSE, and in case of error the</span></span>
<span>                        <span class="co"># the error will be signaled and computation </span></span>
<span>                        <span class="co"># stopped.</span></span>
<span>   .progress <span class="op">=</span> <span class="cn">FALSE</span>  <span class="co"># default is TRUE, and progress bar will be shown.</span></span>
<span> <span class="op">)</span></span>
<span><span class="co">#&gt;                                                                       txt</span></span>
<span><span class="co">#&gt; 1 I'm very satisfied with the course; it was very interesting and useful.</span></span>
<span><span class="co">#&gt; 2                          I didn't like it at all; it was deadly boring.</span></span>
<span><span class="co">#&gt; 3                                     The best course I've ever attended.</span></span>
<span><span class="co">#&gt; 4                                         The course was a waste of time.</span></span>
<span><span class="co">#&gt; 5                                                          blah blah blah</span></span>
<span><span class="co">#&gt; 6                                                                    woow</span></span>
<span><span class="co">#&gt; 7                                                             bim bum bam</span></span>
<span><span class="co">#&gt;       gpt_res</span></span>
<span><span class="co">#&gt; 1   satisfied</span></span>
<span><span class="co">#&gt; 2 unsatisfied</span></span>
<span><span class="co">#&gt; 3   satisfied</span></span>
<span><span class="co">#&gt; 4 unsatisfied</span></span>
<span><span class="co">#&gt; 5        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 6        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 7        &lt;NA&gt;</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="robust-example-with-for-loops-and-error-handling">Robust example with for loops and error handling<a class="anchor" aria-label="anchor" href="#robust-example-with-for-loops-and-error-handling"></a>
</h2>
<p>This example is useful for long computation in which errors from the server-side can happened (maybe after days of querying). The following script will save each result one-by one, so that in case of error the evaluated results won’t be lost.</p>
<p>In case of any error, the error message(s) will be reported as a warning, but it does not stop the computation. Moreover, re-executing the loop will evaluate the queries only where they were failed or not performed yet.</p>
<blockquote>
<p>NOTE: Object not stored (on disk) will still be lost if the session crashes! For maximum robustness, efficiency, and security, it is suggested to transpose the logic onto a <a href="https://docs.ropensci.org/targets/" class="external-link">targets</a> pipeline (see the <a href="https://books.ropensci.org/targets/" class="external-link">manual</a> for this; the idea is to map each record to a branch of a single target, so that a successful query never has to be re-executed.)</p>
</blockquote>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># This is a function that take a text and attach it at the end of the</span></span>
<span><span class="co"># original provided prompt</span></span>
<span></span>
<span><span class="co"># install.packages("depigner")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://corradolanera.github.io/depigner/" class="external-link">depigner</a></span><span class="op">)</span> <span class="co"># for progress bar `pb_len()` and `tick()`</span></span>
<span><span class="co">#&gt; Welcome to depigner: we are here to un-stress you!</span></span>
<span><span class="va">usr_prompter</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/create_usr_data_prompter.html">create_usr_data_prompter</a></span><span class="op">(</span><span class="va">usr_prompt</span>, closing <span class="op">=</span> <span class="va">closing</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">db</span><span class="op">)</span></span>
<span><span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA_character_</span></span>
<span></span>
<span><span class="va">pb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://corradolanera.github.io/depigner/reference/pb_len.html" class="external-link">pb_len</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu">checkmate</span><span class="fu">::</span><span class="fu"><a href="https://mllg.github.io/checkmate/reference/checkScalarNA.html" class="external-link">test_scalar_na</a></span><span class="op">(</span><span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>      prompt <span class="op">=</span> <span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span></span>
<span>        sys_prompt <span class="op">=</span> <span class="va">sys_prompt</span>,</span>
<span>        usr_prompt <span class="op">=</span> <span class="fu">usr_prompter</span><span class="op">(</span><span class="va">db</span><span class="op">[[</span><span class="st">"txt"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>      <span class="op">)</span>,</span>
<span>      na_if_error <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>      <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="fu"><a href="https://corradolanera.github.io/depigner/reference/pb_len.html" class="external-link">tick</a></span><span class="op">(</span><span class="va">pb</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Row"</span>, <span class="va">i</span>, <span class="st">"of"</span>, <span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; evaluated: Row 4 of 7 [================&gt;------------]  57% in  2s [ETA:  2s]evaluated: Row 5 of 7 [====================&gt;--------]  71% in  3s [ETA:  1s]evaluated: Row 6 of 7 [========================&gt;----]  86% in  4s [ETA:  1s]evaluated: Row 7 of 7 [=============================] 100% in  5s [ETA:  0s]</span></span>
<span></span>
<span><span class="va">db</span></span>
<span><span class="co">#&gt;                                                                       txt</span></span>
<span><span class="co">#&gt; 1 I'm very satisfied with the course; it was very interesting and useful.</span></span>
<span><span class="co">#&gt; 2                          I didn't like it at all; it was deadly boring.</span></span>
<span><span class="co">#&gt; 3                                     The best course I've ever attended.</span></span>
<span><span class="co">#&gt; 4                                         The course was a waste of time.</span></span>
<span><span class="co">#&gt; 5                                                          blah blah blah</span></span>
<span><span class="co">#&gt; 6                                                                    woow</span></span>
<span><span class="co">#&gt; 7                                                             bim bum bam</span></span>
<span><span class="co">#&gt;       gpt_res</span></span>
<span><span class="co">#&gt; 1   satisfied</span></span>
<span><span class="co">#&gt; 2 unsatisfied</span></span>
<span><span class="co">#&gt; 3   satisfied</span></span>
<span><span class="co">#&gt; 4 unsatisfied</span></span>
<span><span class="co">#&gt; 5        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 6        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 7        &lt;NA&gt;</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="base-chatgpt-prompt-creation-not-for-api">Base ChatGPT prompt creation (NOT for API)<a class="anchor" aria-label="anchor" href="#base-chatgpt-prompt-creation-not-for-api"></a>
</h2>
<p>You can use the <code>compose_prompt</code> function to create a prompt for ChatGPT. This function is useful because it helps you to compose the prompt following best practices in composing prompt. In fact the arguments are just the main components every good prompt should have. They do just that, composing the prompt for you juxtaposing the components in the right order.</p>
<blockquote>
<p>WARNING: The result is suitable to be copy-pasted on ChatGPT, not to be used with API calls, i.e., it cannot be used with the <code>query_gpt</code> function!</p>
</blockquote>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">chat_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_prompt</a></span><span class="op">(</span></span>
<span>  role <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  context <span class="op">=</span> <span class="st">"You are analyzing the comments of the students of the last course."</span>,</span>
<span>  task <span class="op">=</span> <span class="st">"Your task is to extract information from a text provided."</span>,</span>
<span>  instructions <span class="op">=</span> <span class="st">"You should extract the first and last words of the text."</span>,</span>
<span>  output <span class="op">=</span> <span class="st">"Return the first and last words of the text separated by a dash, i.e., `first - last`."</span>,</span>
<span>  style <span class="op">=</span> <span class="st">"Do not add any additional information, return only the requested information."</span>,</span>
<span>  examples <span class="op">=</span> <span class="st">"</span></span>
<span><span class="st">    # Examples:</span></span>
<span><span class="st">    text: 'This is an example text.'</span></span>
<span><span class="st">    output: 'This - text'</span></span>
<span><span class="st">    text: 'Another example text!!!'</span></span>
<span><span class="st">    output: 'Another - text'"</span>,</span>
<span>  text <span class="op">=</span> <span class="st">"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">chat_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; You are the assistant of a university professor.</span></span>
<span><span class="co">#&gt; You are analyzing the comments of the students of the last course.</span></span>
<span><span class="co">#&gt; Your task is to extract information from a text provided.</span></span>
<span><span class="co">#&gt; You should extract the first and last words of the text.</span></span>
<span><span class="co">#&gt; Return the first and last words of the text separated by a dash, i.e., `first - last`.</span></span>
<span><span class="co">#&gt; Do not add any additional information, return only the requested information.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     # Examples:</span></span>
<span><span class="co">#&gt;     text: 'This is an example text.'</span></span>
<span><span class="co">#&gt;     output: 'This - text'</span></span>
<span><span class="co">#&gt;     text: 'Another example text!!!'</span></span>
<span><span class="co">#&gt;     output: 'Another - text'</span></span>
<span><span class="co">#&gt; """"</span></span>
<span><span class="co">#&gt; Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura</span></span>
<span><span class="co">#&gt; """"</span></span></code></pre></div>
<figure><img src="dev/img/gpt-example.png" alt="https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d"><figcaption aria-hidden="true"><a href="https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d" class="external-link uri">https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d</a>
</figcaption></figure>
</div>
<div class="section level2">
<h2 id="other-options-and-utilities">Other options and utilities<a class="anchor" aria-label="anchor" href="#other-options-and-utilities"></a>
</h2>
<div class="section level3">
<h3 id="options-for-temperature-max_tokens-and-seed">Options for <code>temperature</code>, <code>max_tokens</code>, and <code>seed</code>
<a class="anchor" aria-label="anchor" href="#options-for-temperature-max_tokens-and-seed"></a>
</h3>
<p>You cannot use all the features of the official APIs here (<a href="https://platform.openai.com/docs/api-reference/chat/create" class="external-link uri">https://platform.openai.com/docs/api-reference/chat/create</a>), we have selected the following to be available here to keep the interface in an opinionated balance between ease of use, efficiency and flexibility (please contact the authors if you need more):</p>
<ul>
<li>
<code>temperature</code>: “What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.”</li>
<li>
<code>max_tokens</code>: “The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length.”</li>
<li>
<code>seed</code>, “This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.”</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    temperature <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>    max_tokens <span class="op">=</span> <span class="fl">30</span>,</span>
<span>    seed <span class="op">=</span> <span class="fl">1234</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span> <span class="co"># limited to 30 tokens!</span></span>
<span><span class="co">#&gt; As an AI, I'm not able to provide specific information about personal experiences or past courses since I don't have the capability to conduct courses or retain personal memories</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="pythons-backend">Python’s backend<a class="anchor" aria-label="anchor" href="#pythons-backend"></a>
</h3>
<p>Often, for complex prompt it happens that the R environment (everyone we have experimented, i.e. <a href="https://github.com/irudnyts/openai" class="external-link">openai</a>, <a href="https://httr.r-lib.org/" class="external-link">httr</a>, <a href="https://httr2.r-lib.org" class="external-link">httr2</a>, and <code>curl</code>) return a timeout error for a certificate validation (see, e.g.: <a href="https://github.com/irudnyts/openai/issues/61" class="external-link uri">https://github.com/irudnyts/openai/issues/61</a>, and <a href="https://github.com/irudnyts/openai/issues/42" class="external-link uri">https://github.com/irudnyts/openai/issues/42</a>). The same does not happen with a pure python backend using the official OpenAI’s <a href="https://github.com/irudnyts/openai" class="external-link">openai</a> library. you can setup a Python backend by executing <code><a href="reference/setup_py.html">setup_py()</a></code>, and setting <code>use_py = TRUE</code> in the functions that send the queries (i.e., <code>query_gpt</code>, <code>query_gpt_on_column</code>, and <code>get_completion_from_messages</code>)</p>
<blockquote>
<p>NOTE: using a Python backend can be a little slower, but sometimes necessary.</p>
</blockquote>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/setup_py.html">setup_py</a></span><span class="op">(</span>ask <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="co"># default TRUE will always ask for confirmation.</span></span>
<span><span class="co">#&gt; virtualenv: r-gpt-venv</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    use_py <span class="op">=</span> <span class="cn">TRUE</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; As an AI, I don't have personal experiences or the ability to teach courses myself. However, I can help you design a course, provide information on course content, or assist with any specific topics you might be interested in. If you have a particular subject in mind, please let me know, and I can provide relevant information or resources!</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="personalized-servers-endpoint">Personalized server’s endpoint<a class="anchor" aria-label="anchor" href="#personalized-servers-endpoint"></a>
</h3>
<p>If you have a personal server that listens for queries using the OpenAI API format, (e.g. using LM Studio, with open source models), you can set the endpoint to POST the query to your server instead of the OpenAI one.</p>
<blockquote>
<p>NOTE: if you are using a personalised server endpoint, you can select the model you whish to use in the usual way, i.e., using the <code>model</code> option. Of course, the available models you can select depend on your local server configuration.</p>
</blockquote>
<blockquote>
<p>WARNING: this option cannot be selected if the Python backend is requested (i.e., setting both <code>use_py = TRUE</code> and a custom <code>endpoint</code> won’t work)!</p>
</blockquote>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># we do not run this in the README</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    endopont <span class="op">=</span> <span class="st">"http://localhost:1234/v1/chat/completions"</span>,</span>
<span>    model <span class="op">=</span> <span class="st">"lmstudio-ai/gemma-2b-it-GGUF"</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="batch-openai-requests">Batch OpenAI requests<a class="anchor" aria-label="anchor" href="#batch-openai-requests"></a>
</h2>
<p>By April 23, 2024, OpenAI has introduced a new feature that allows you to send multiple requests in a single call (see: <a href="https://openai.com/index/more-enterprise-grade-features-for-api-customers/" class="external-link uri">https://openai.com/index/more-enterprise-grade-features-for-api-customers/</a>).</p>
<p>This feature is now available in <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a>. You can use the <code>batch_*</code> functions to send multiple requests in a single call. The functions are <code>file_upload</code>,</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a list of prompts</span></span>
<span><span class="va">sys_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_sys_prompt</a></span><span class="op">(</span><span class="st">"You are a funny assistant."</span><span class="op">)</span></span>
<span><span class="va">usr_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_usr_prompt</a></span><span class="op">(</span></span>
<span>  <span class="st">"Tell me a joke ending in:"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">prompter</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/create_usr_data_prompter.html">create_usr_data_prompter</a></span><span class="op">(</span>usr_prompt <span class="op">=</span> <span class="va">usr_prompt</span><span class="op">)</span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"deadly boring!"</span>,</span>
<span>    <span class="st">"A bit boring, but interesting"</span>,</span>
<span>    <span class="st">"How nice, I loved it!"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="va">text</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span></span>
<span>    \<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span></span>
<span>      sys_prompt <span class="op">=</span> <span class="va">sys_prompt</span>,</span>
<span>      usr_prompt <span class="op">=</span> <span class="fu">prompter</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a jsonl file as required by the API, and save it</span></span>
<span><span class="va">jsonl_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/create_jsonl_records.html">create_jsonl_records</a></span><span class="op">(</span><span class="va">prompts</span><span class="op">)</span></span>
<span><span class="va">out_jsonl_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/write_jsonl_files.html">write_jsonl_files</a></span><span class="op">(</span><span class="va">jsonl_text</span>, <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># upload the jsonl file to OpenAI project</span></span>
<span><span class="co"># The project used is the one linked with the API key you have set in</span></span>
<span><span class="co"># the environment variable `OPENAI_API_KEY`</span></span>
<span><span class="va">batch_file_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/file_upload.html">file_upload</a></span><span class="op">(</span><span class="va">out_jsonl_path</span><span class="op">)</span></span>
<span><span class="va">batch_file_info</span></span>
<span><span class="co">#&gt; # A tibble: 1 × 8</span></span>
<span><span class="co">#&gt;   object id              purpose filename bytes created_at status status_details</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;      &lt;int&gt; &lt;chr&gt;  &lt;lgl&gt;         </span></span>
<span><span class="co">#&gt; 1 file   file-9tzaTpyjx… batch   2024071…   847 1721405632 proce… NA</span></span>
<span></span>
<span><span class="co"># Create a batch job from the id of an uploaded jsonl file</span></span>
<span><span class="va">batch_job_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_create.html">batch_create</a></span><span class="op">(</span><span class="va">batch_file_info</span><span class="op">[[</span><span class="st">"id"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">batch_job_info</span></span>
<span><span class="co">#&gt; # A tibble: 1 × 22</span></span>
<span><span class="co">#&gt;   id               object endpoint errors input_file_id completion_window status</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;    &lt;lgl&gt;  &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt; </span></span>
<span><span class="co">#&gt; 1 batch_llpfVYB82… batch  /v1/cha… NA     file-9tzaTpy… 24h               valid…</span></span>
<span><span class="co">#&gt; # ℹ 15 more variables: output_file_id &lt;lgl&gt;, error_file_id &lt;lgl&gt;,</span></span>
<span><span class="co">#&gt; #   created_at &lt;int&gt;, in_progress_at &lt;lgl&gt;, expires_at &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   finalizing_at &lt;lgl&gt;, completed_at &lt;lgl&gt;, failed_at &lt;lgl&gt;, expired_at &lt;lgl&gt;,</span></span>
<span><span class="co">#&gt; #   cancelling_at &lt;lgl&gt;, cancelled_at &lt;lgl&gt;, request_counts_total &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   request_counts_completed &lt;int&gt;, request_counts_failed &lt;int&gt;, metadata &lt;lgl&gt;</span></span>
<span></span>
<span><span class="co"># You can retrieve the status of the batch job by its ID</span></span>
<span><span class="va">batch_status</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_status.html">batch_status</a></span><span class="op">(</span><span class="va">batch_job_info</span><span class="op">[[</span><span class="st">"id"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">batch_status</span></span>
<span><span class="co">#&gt; # A tibble: 1 × 22</span></span>
<span><span class="co">#&gt;   id               object endpoint errors input_file_id completion_window status</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;    &lt;lgl&gt;  &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt; </span></span>
<span><span class="co">#&gt; 1 batch_llpfVYB82… batch  /v1/cha… NA     file-9tzaTpy… 24h               valid…</span></span>
<span><span class="co">#&gt; # ℹ 15 more variables: output_file_id &lt;lgl&gt;, error_file_id &lt;lgl&gt;,</span></span>
<span><span class="co">#&gt; #   created_at &lt;int&gt;, in_progress_at &lt;lgl&gt;, expires_at &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   finalizing_at &lt;lgl&gt;, completed_at &lt;lgl&gt;, failed_at &lt;lgl&gt;, expired_at &lt;lgl&gt;,</span></span>
<span><span class="co">#&gt; #   cancelling_at &lt;lgl&gt;, cancelled_at &lt;lgl&gt;, request_counts_total &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   request_counts_completed &lt;int&gt;, request_counts_failed &lt;int&gt;, metadata &lt;lgl&gt;</span></span>
<span></span>
<span><span class="co"># You can list all the batches in the project (default limit is 10)</span></span>
<span><span class="va">list_of_batches</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_list.html">batch_list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">list_of_batches</span></span>
<span><span class="co">#&gt; # A tibble: 10 × 5</span></span>
<span><span class="co">#&gt;    object data$id            $object $endpoint $errors first_id last_id has_more</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;     &lt;lgl&gt;   &lt;chr&gt;    &lt;chr&gt;   &lt;lgl&gt;   </span></span>
<span><span class="co">#&gt;  1 list   batch_llpfVYB82OH… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  2 list   batch_c2nKNs3J5d4… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  3 list   batch_cObMy854Rak… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  4 list   batch_6iJ7tRhs3yT… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  5 list   batch_S1fdlTALARX… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  6 list   batch_KhN6KOkw0PI… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  7 list   batch_abg57yt8m8B… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  8 list   batch_V6F3K1gsPJq… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt;  9 list   batch_ojZskHmd5BB… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt; 10 list   batch_OfsBXK9Hm9Z… batch   /v1/chat… NA      batch_l… batch_… TRUE    </span></span>
<span><span class="co">#&gt; # ℹ 16 more variables: data$input_file_id &lt;chr&gt;, $completion_window &lt;chr&gt;,</span></span>
<span><span class="co">#&gt; #   $status &lt;chr&gt;, $output_file_id &lt;chr&gt;, $error_file_id &lt;chr&gt;,</span></span>
<span><span class="co">#&gt; #   $created_at &lt;int&gt;, $in_progress_at &lt;int&gt;, $expires_at &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   $finalizing_at &lt;int&gt;, $completed_at &lt;int&gt;, $failed_at &lt;lgl&gt;,</span></span>
<span><span class="co">#&gt; #   $expired_at &lt;lgl&gt;, $cancelling_at &lt;int&gt;, $cancelled_at &lt;int&gt;,</span></span>
<span><span class="co">#&gt; #   $request_counts &lt;df[,3]&gt;, $metadata &lt;lgl&gt;</span></span>
<span></span>
<span><span class="kw">while</span> <span class="op">(</span><span class="va">batch_status</span><span class="op">[[</span><span class="st">"status"</span><span class="op">]</span><span class="op">]</span> <span class="op">!=</span> <span class="st">"completed"</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Sys.sleep.html" class="external-link">Sys.sleep</a></span><span class="op">(</span><span class="fl">60</span><span class="op">)</span></span>
<span>  <span class="va">batch_status</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_status.html">batch_status</a></span><span class="op">(</span><span class="va">batch_job_info</span><span class="op">[[</span><span class="st">"id"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Waiting for the batch to be completed...\n"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Waiting for the batch to be completed...</span></span>
<span><span class="co">#&gt; Waiting for the batch to be completed...</span></span>
<span></span>
<span><span class="co"># Once the batch is completed, you can retrieve the results by</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_result.html">batch_result</a></span><span class="op">(</span><span class="va">batch_status</span><span class="op">[[</span><span class="st">"id"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">results</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 3</span></span>
<span><span class="co">#&gt;  $ :List of 7</span></span>
<span><span class="co">#&gt;   ..$ id                : chr "chatcmpl-9mkYAhlZRMYzyTvf1Jj4YrP8m5sPG"</span></span>
<span><span class="co">#&gt;   ..$ object            : chr "chat.completion"</span></span>
<span><span class="co">#&gt;   ..$ created           : int 1721405710</span></span>
<span><span class="co">#&gt;   ..$ model             : chr "gpt-4o-mini-2024-07-18"</span></span>
<span><span class="co">#&gt;   ..$ choices           :'data.frame':   1 obs. of  4 variables:</span></span>
<span><span class="co">#&gt;   ..$ usage             :List of 3</span></span>
<span><span class="co">#&gt;   ..$ system_fingerprint: chr "fp_7dd529cfca"</span></span>
<span><span class="co">#&gt;  $ :List of 7</span></span>
<span><span class="co">#&gt;   ..$ id                : chr "chatcmpl-9mkXkKyV6BnlG2p9qWMK5NFhqp8Oy"</span></span>
<span><span class="co">#&gt;   ..$ object            : chr "chat.completion"</span></span>
<span><span class="co">#&gt;   ..$ created           : int 1721405684</span></span>
<span><span class="co">#&gt;   ..$ model             : chr "gpt-4o-mini-2024-07-18"</span></span>
<span><span class="co">#&gt;   ..$ choices           :'data.frame':   1 obs. of  4 variables:</span></span>
<span><span class="co">#&gt;   ..$ usage             :List of 3</span></span>
<span><span class="co">#&gt;   ..$ system_fingerprint: chr "fp_7dd529cfca"</span></span>
<span><span class="co">#&gt;  $ :List of 7</span></span>
<span><span class="co">#&gt;   ..$ id                : chr "chatcmpl-9mkYRVzd0ORHzO3NBo2IZMDSbjYkk"</span></span>
<span><span class="co">#&gt;   ..$ object            : chr "chat.completion"</span></span>
<span><span class="co">#&gt;   ..$ created           : int 1721405727</span></span>
<span><span class="co">#&gt;   ..$ model             : chr "gpt-4o-mini-2024-07-18"</span></span>
<span><span class="co">#&gt;   ..$ choices           :'data.frame':   1 obs. of  4 variables:</span></span>
<span><span class="co">#&gt;   ..$ usage             :List of 3</span></span>
<span><span class="co">#&gt;   ..$ system_fingerprint: chr "fp_8b761cb050"</span></span>
<span></span>
<span><span class="co"># By default the results are simplified to the response body returning </span></span>
<span><span class="co"># a list of responses, so you can continue to work as usual. If you want</span></span>
<span><span class="co"># to have the full response, you can set `simplify = FALSE` in the</span></span>
<span><span class="co"># `batch_result` call.</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map_chr</a></span><span class="op">(</span><span class="va">results</span>, <span class="va">get_content</span><span class="op">)</span></span>
<span><span class="va">res</span></span>
<span><span class="co">#&gt; [1] "Why did the scarecrow win an award?\n\nBecause he was outstanding in his field... but his speeches were just deadly boring!"                                             </span></span>
<span><span class="co">#&gt; [2] "Why did the librarian get kicked off the plane?\n\nBecause it was overbooked, and she kept trying to check out the in-flight magazine!\n\nA bit boring, but interesting."</span></span>
<span><span class="co">#&gt; [3] "Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!\n\nHow nice, I loved it!"</span></span>
<span></span>
<span><span class="co"># You can cancel a batch job by its ID (if it isn't completed yet)</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># the batch is completed now so this would raise an error</span></span>
<span>  <span class="va">batch_cancelled</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/batch_cancel.html">batch_cancel</a></span><span class="op">(</span><span class="va">batch_job_info</span><span class="op">[[</span><span class="st">"id"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">batch_cancelled</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="code-of-conduct">Code of Conduct<a class="anchor" aria-label="anchor" href="#code-of-conduct"></a>
</h2>
<p>Please note that the <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a> project is released with a <a href="https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html" class="external-link">Contributor Code of Conduct</a>. By contributing to this project, you agree to abide by its terms.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/CorradoLanera/gpteasyr/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/CorradoLanera/gpteasyr/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>

<div class="community">
<h2 data-toc-skip>Community</h2>
<ul class="list-unstyled">
<li><a href="CODE_OF_CONDUCT.html">Code of conduct</a></li>
</ul>
</div>

<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing gpteasyr</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Corrado Lanera <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0002-0520-7428" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a></li>
<li><a href="https://app.codecov.io/gh/CorradoLanera/gpteasyr?branch=main" class="external-link"><img src="https://codecov.io/gh/CorradoLanera/gpteasyr/branch/main/graph/badge.svg" alt="Codecov test coverage"></a></li>
<li><a href="https://github.com/CorradoLanera/gpteasyr/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/CorradoLanera/gpteasyr/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Corrado Lanera.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
