[{"path":[]},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement Corrado.Lanera@ubep.unipd.. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 CorradoLanera Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Corrado Lanera. Author, maintainer.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lanera C (2024). ubep.gpt: basic simple interface OpenAI’s GPT API. R package version 0.2.9,  https://ubesp-dctv.github.io/ubep.gpt/, https://github.com/UBESP-DCTV/ubep.gpt.","code":"@Manual{,   title = {ubep.gpt: A basic and simple interface to OpenAI’s GPT API},   author = {Corrado Lanera},   year = {2024},   note = {R package version 0.2.9,  https://ubesp-dctv.github.io/ubep.gpt/},   url = {https://github.com/UBESP-DCTV/ubep.gpt}, }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"ubepgpt","dir":"","previous_headings":"","what":"A basic and simple interface to OpenAI’s GPT API","title":"A basic and simple interface to OpenAI’s GPT API","text":"goal ubep.gpt provide basic/simple interface OpenAI’s GPT API. package designed work dataframes/tibbles simplify process querying API.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A basic and simple interface to OpenAI’s GPT API","text":"can install development version ubep.gpt like :","code":"remotes::install_github(\"UBESP-DCTV/ubep.gpt\")"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic example","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use query_gpt function query GPT API. can decide model use (e.g., gpt-3.5-turbo, gpt-4-turbo, gpt-4o). function useful mainly iterate query decided number times (10 default) case error (often caused server overload). use function need compose prompt. can use (necessary!) compose_prompt_api function compose prompt properly optional (single) system prompt (.e., gpt’s setup) (single) user prompt (.e., query). function useful helps compose prompt automatically adopting required API’s structure. NOTE: can still pass correctly formatted list (lists) described official documentation (https://platform.openai.com/docs/api-reference/chat). queried API, can extract content response using get_content function. can also extract tokens prompt response using get_tokens function.","code":"library(ubep.gpt) #> Wellcome to ubep.gpt! #> The OPENAI_API_KEY environment variable is set #> You are ready to use the package `ubep.gpt`. #> Just, double check if the key is the correct one. #> REMIND: Never share your API key with others. #>       Keep it safe and secure. #>       If you think that your API key was compromised, #>       you can regenerate it in the OpenAI-API website #>       (https://platform.openai.com/api-keys). #> Enjoy the package! #> If you like to use the python backend (working only for GPT's OpenAI requests!), #> setup the environmen first by executing: #> `setup_py()`(default virtual environment name is 'r-gpt-venv'). #> If you need to change the default name, run: #> `setup_py(\"<your_custom_environment_name>\")` prompt <- compose_prompt_api(   sys_prompt = \"You are the assistant of a university professor.\",   usr_prompt = \"Tell me about the last course you provided.\" ) prompt #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"You are the assistant of a university professor.\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"Tell me about the last course you provided.\"  res <- query_gpt(   prompt = prompt,   model = \"gpt-3.5-turbo\",   quiet = FALSE, # default TRUE   max_try = 2, # default 10   temperature = 1.5, # default 0 [0-2]   max_tokens = 100 # default 1000 ) #> ℹ Total tries: 1. #> ℹ Prompt token used: 29. #> ℹ Response token used: 100. #> ℹ Total token used: 129.  str(res) #> List of 7 #>  $ id                : chr \"chatcmpl-9PzusvOFD57oVSnHWcp3JKk9Kr75U\" #>  $ object            : chr \"chat.completion\" #>  $ created           : int 1715983234 #>  $ model             : chr \"gpt-3.5-turbo-0125\" #>  $ choices           :'data.frame':  1 obs. of  5 variables: #>   ..$ index          : int 0 #>   ..$ logprobs       : logi NA #>   ..$ finish_reason  : chr \"length\" #>   ..$ message.role   : chr \"assistant\" #>   ..$ message.content: chr \"The last course that our professor provided was on Advanced Machine Learning. This course delved into more comp\"| __truncated__ #>  $ usage             :List of 3 #>   ..$ prompt_tokens    : int 29 #>   ..$ completion_tokens: int 100 #>   ..$ total_tokens     : int 129 #>  $ system_fingerprint: NULL get_content(res) #> [1] \"The last course that our professor provided was on Advanced Machine Learning. This course delved into more complex machine learning techniques such as deep learning, reinforcement learning, and unsupervised learning. Students learned about cutting-edge algorithms and applications in areas such as natural language processing, computer vision, and recommendation systems. The course also had a significant practical component with coding assignments and a final project where students applied their knowledge to analyze real-world datasets. It was a challenging yet engaging course that provided students with valuable skills for\"  # for a well formatted output on R, use `cat()` get_content(res) |> cat() #> The last course that our professor provided was on Advanced Machine Learning. This course delved into more complex machine learning techniques such as deep learning, reinforcement learning, and unsupervised learning. Students learned about cutting-edge algorithms and applications in areas such as natural language processing, computer vision, and recommendation systems. The course also had a significant practical component with coding assignments and a final project where students applied their knowledge to analyze real-world datasets. It was a challenging yet engaging course that provided students with valuable skills for  get_tokens(res) #> [1] 129 get_tokens(res, \"prompt\") #> [1] 29 get_tokens(res, \"all\") #>     prompt_tokens completion_tokens      total_tokens  #>                29               100               129"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"easy-prompt-assisted-creation","dir":"","previous_headings":"","what":"Easy prompt-assisted creation","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use compose_sys_prompt compose_usr_prompt functions create system user prompts, respectively. functions useful help compose prompts following best practices composing prompt. fact arguments just main components every good prompt . just , composing prompt juxtaposing components order.","code":"sys_prompt <- compose_sys_prompt(   role = \"You are the assistant of a university professor.\",   context = \"You are analyzing the comments of the students of the last course.\" ) cat(sys_prompt) #> You are the assistant of a university professor. #> You are analyzing the comments of the students of the last course.  usr_prompt <- compose_usr_prompt(   task = \"Your task is to extract information from a text provided.\",   instructions = \"You should extract the first and last words of the text.\",   output = \"Return the first and last words of the text separated by a dash, i.e., `first - last`.\",   style = \"Do not add any additional information, return only the requested information.\",   examples = \"     # Examples:     text: 'This is an example text.'     output: 'This - text'     text: 'Another example text!!!'     output: 'Another - text'\",   text = \"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\",   closing = \"Take a deep breath and work on the problem step-by-step.\" ) cat(usr_prompt) #> Your task is to extract information from a text provided. #> You should extract the first and last words of the text. #> Return the first and last words of the text separated by a dash, i.e., `first - last`. #> Do not add any additional information, return only the requested information. #>  #>     # Examples: #>     text: 'This is an example text.' #>     output: 'This - text' #>     text: 'Another example text!!!' #>     output: 'Another - text' #> \"\"\" #> Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura #> \"\"\" #> Take a deep breath and work on the problem step-by-step.  compose_prompt_api(sys_prompt, usr_prompt) |>    query_gpt() |>    get_content() #> [1] \"Nel - oscura\""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"querying-a-column-of-a-dataframe","dir":"","previous_headings":"","what":"Querying a column of a dataframe","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use query_gpt_on_column function query GPT API column dataframe. function useful helps iterate query row column compose prompt automatically adopting required API’s structure. case, need provide components prompt creating prompt template, name column embed template “text” query. prompt’s components optional, can provide ones need: role context compose system prompt, task, instructions, output, style, examples compose user prompt (just juxtaposed right order)","code":"db <- data.frame(   txt = c(     \"I'm very satisfied with the course; it was very interesting and useful.\",     \"I didn't like it at all; it was deadly boring.\",     \"The best course I've ever attended.\",     \"The course was a waste of time.\",     \"blah blah blah\",     \"woow\",     \"bim bum bam\"   ) )  # system role <- \"You are the assistant of a university professor.\" context <- \"You are analyzing the comments of the students of the last course.\"  # user task <- \"Your task is to understand if they are satisfied with the course.\" instructions <- \"Analyze the comments and decide if they are satisfied or not.\" output <- \"Report 'satisfied' or 'unsatisfied', in case of doubt or impossibility report 'NA'.\" style <- \"Do not add any comment, return only and exclusively one of the possible classifications.\"  examples <- \"   # Examples:   text: 'I'm very satisfied with the course; it was very interesting and useful.'   output: 'satisfied'   text: 'I didn't like it at all; it was deadly boring.'   output: 'unsatisfied'\"  closing <- \"Take a deep breath and work on the problem step-by-step.\" # This will be added AFTER the embedded text  sys_prompt <- compose_sys_prompt(role = role, context = context) usr_prompt <- compose_usr_prompt(   task = task,   instructions = instructions,   output = output,   style = style,   examples = examples   # don't put the `closing` here if you want to use it on   # `query_gpt_on_column` after the embedded text;   # if here, it will go after the examples but before the embedded text. )  db |>  query_gpt_on_column(    text_column = \"txt\",  # the name of the column containing the text to                          # analyze after being embedded in the prompt.    sys_prompt = sys_prompt,    usr_prompt = usr_prompt,    closing = closing,  # this will be added AFTER the embedded text    na_if_error = TRUE,  # dafault is FALSE, and in case of error the                         # the error will be signaled and computation                          # stopped.    .progress = FALSE  # default is TRUE, and progress bar will be shown.  ) #>                                                                       txt #> 1 I'm very satisfied with the course; it was very interesting and useful. #> 2                          I didn't like it at all; it was deadly boring. #> 3                                     The best course I've ever attended. #> 4                                         The course was a waste of time. #> 5                                                          blah blah blah #> 6                                                                    woow #> 7                                                             bim bum bam #>       gpt_res #> 1   satisfied #> 2 unsatisfied #> 3   satisfied #> 4 unsatisfied #> 5        <NA> #> 6        <NA> #> 7        <NA>"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"robust-example-with-for-loops-and-error-handling","dir":"","previous_headings":"","what":"Robust example with for loops and error handling","title":"A basic and simple interface to OpenAI’s GPT API","text":"example useful long computation errors server-side can happened (maybe days querying). following script save result one-one, case error evaluated results won’t lost. case error, error message(s) reported warning, stop computation. Moreover, re-executing loop evaluate queries failed performed yet.","code":"# This is a function that take a text and attach it at the end of the # original provided prompt  # install.packages(\"depigner\") library(depigner) # for progress bar `pb_len()` and `tick()` #> Welcome to depigner: we are here to un-stress you! usr_prompter <- create_usr_data_prompter(usr_prompt, closing = closing)  n <- nrow(db) db[[\"gpt_res\"]] <- NA_character_  pb <- pb_len(n) for (i in seq_len(n)) {   if (checkmate::test_scalar_na(db[[\"gpt_res\"]][[i]])) {     db[[\"gpt_res\"]][[i]] <- query_gpt(       prompt = compose_prompt_api(         sys_prompt = sys_prompt,         usr_prompt = usr_prompter(db[[\"txt\"]][[i]])       ),       na_if_error = TRUE     ) |>        get_content()   }   tick(pb, paste(\"Row\", i, \"of\", n)) }  db #>                                                                       txt #> 1 I'm very satisfied with the course; it was very interesting and useful. #> 2                          I didn't like it at all; it was deadly boring. #> 3                                     The best course I've ever attended. #> 4                                         The course was a waste of time. #> 5                                                          blah blah blah #> 6                                                                    woow #> 7                                                             bim bum bam #>       gpt_res #> 1   satisfied #> 2 unsatisfied #> 3   satisfied #> 4 unsatisfied #> 5        <NA> #> 6        <NA> #> 7        <NA>"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"base-chatgpt-prompt-creation-not-for-api","dir":"","previous_headings":"","what":"Base ChatGPT prompt creation (NOT for API)","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use compose_prompt function create prompt ChatGPT. function useful helps compose prompt following best practices composing prompt. fact arguments just main components every good prompt . just , composing prompt juxtaposing components right order. WARNING: result suitable copy-pasted ChatGPT, used API calls, .e., used query_gpt function! https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d","code":"chat_prompt <- compose_prompt(   role = \"You are the assistant of a university professor.\",   context = \"You are analyzing the comments of the students of the last course.\",   task = \"Your task is to extract information from a text provided.\",   instructions = \"You should extract the first and last words of the text.\",   output = \"Return the first and last words of the text separated by a dash, i.e., `first - last`.\",   style = \"Do not add any additional information, return only the requested information.\",   examples = \"     # Examples:     text: 'This is an example text.'     output: 'This - text'     text: 'Another example text!!!'     output: 'Another - text'\",   text = \"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\" )  cat(chat_prompt) #> You are the assistant of a university professor. #> You are analyzing the comments of the students of the last course. #> Your task is to extract information from a text provided. #> You should extract the first and last words of the text. #> Return the first and last words of the text separated by a dash, i.e., `first - last`. #> Do not add any additional information, return only the requested information. #>  #>     # Examples: #>     text: 'This is an example text.' #>     output: 'This - text' #>     text: 'Another example text!!!' #>     output: 'Another - text' #> \"\"\"\" #> Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura #> \"\"\"\""},{"path":[]},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"options-for-temperature-max_tokens-and-seed","dir":"","previous_headings":"Other options and utilities","what":"Options for temperature, max_tokens, and seed","title":"A basic and simple interface to OpenAI’s GPT API","text":"use option official APIs (https://platform.openai.com/docs/api-reference/chat/create), select following available (please contact authors need ): temperature: “sampling temperature use, 0 2. Higher values like 0.8 make output random, lower values like 0.2 make focused deterministic.” max_tokens: “maximum number tokens can generated chat completion. total length input tokens generated tokens limited model’s context length.” seed, “feature Beta. specified, system make best effort sample deterministically, repeated requests seed parameters return result. Determinism guaranteed, refer system_fingerprint response parameter monitor changes backend.”","code":"res <- query_gpt(     prompt = prompt,     temperature = 1.2,     max_tokens = 30,     seed = 1234  ) |>    get_content()   cat(res) # limited to 30 tokens! #> The last course my professor provided was a graduate-level seminar on cutting-edge research topics in environmental science. The course covered a range of interdisciplinary subjects related to"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"pythons-backend","dir":"","previous_headings":"Other options and utilities","what":"Python’s backend","title":"A basic and simple interface to OpenAI’s GPT API","text":"Often, complex prompt happens R environment (everyone experimented, .e. openai, httr, httr2, curl) return timeout error certificate validation (see, e.g.: https://github.com/irudnyts/openai/issues/61, https://github.com/irudnyts/openai/issues/42). happen pure python backend using official OpenAI’s openai library. can setup Python backend executing setup_py(), setting use_py = TRUE functions send queries (.e., query_gpt, query_gpt_on_column, get_completion_from_messages) NOTE: using Python backend can little slower, sometimes necessary.","code":"setup_py(ask = FALSE) #> virtualenv: r-gpt-venv  res <- query_gpt(     prompt = prompt,     use_py = TRUE  ) |>    get_content()   cat(res) #> The last course that the professor provided was a graduate-level seminar on \"Advanced Topics in Artificial Intelligence.\" The course covered cutting-edge research in areas such as deep learning, natural language processing, and reinforcement learning. Students were required to read and present research papers, participate in discussions, and complete a final project applying the concepts learned in the course. The professor also invited guest speakers from industry and academia to share their expertise and insights with the students. Overall, the course was well-received by the students and provided them with valuable knowledge and skills in the field of artificial intelligence."},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"personalized-servers-endpoint","dir":"","previous_headings":"Other options and utilities","what":"Personalized server’s endpoint","title":"A basic and simple interface to OpenAI’s GPT API","text":"personal server asking queries using OpenAI’s API format, (e.g. using LM Studio, open source models), can set endpoint POST query server instead OpenaAI one. NOTE: using personalized server endpoint, can select model like use usual model option. Clearly, available models depend local server configuration. WARNING: option select Python backend request (.e., setting use_py = TRUE, custom endpoint won’t work)!","code":"if (FALSE) { # we do not run this in the README   res <- query_gpt(     prompt = prompt,     endopont = \"http://localhost:1234/v1/chat/completions\",     model = \"lmstudio-ai/gemma-2b-it-GGUF\"  ) |>    get_content()   cat(res) }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"A basic and simple interface to OpenAI’s GPT API","text":"Please note ubep.gpt project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a prompt to ChatGPT — compose_prompt","title":"Create a prompt to ChatGPT — compose_prompt","text":"function simple wrapper compose good prompt ChatGPT. output nothing juxtaposition separate lines various components (additional text enclosed delimiters bottom prompt). use focused useful remembering getting used entering components useful good prompt.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a prompt to ChatGPT — compose_prompt","text":"","code":"compose_prompt(   role = NULL,   context = NULL,   task = NULL,   instructions = NULL,   output = NULL,   style = NULL,   examples = NULL,   text = NULL,   closing = NULL,   delimiter = if (is.null(text)) NULL else \"\\\"\\\"\\\"\\\"\" )  compose_sys_prompt(role = NULL, context = NULL)  compose_usr_prompt(   task = NULL,   instructions = NULL,   output = NULL,   style = NULL,   examples = NULL,   text = NULL,   closing = NULL,   delimiter = if (is.null(text)) NULL else \"\\\"\\\"\\\"\" )"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a prompt to ChatGPT — compose_prompt","text":"role (chr) role ChatGPT play context (chr) context behind task required task (chr) tasks ChatGPT assess instructions (chr) Description steps ChatGPT follow output (chr) type/kind output required style (chr) style ChatGPT use output examples (chr) examples correct output text (chr) Additional text embed prompt closing (chr) Text include end prompt delimiter (chr) delimiters text embed, sequence three identical symbols suggested","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a prompt to ChatGPT — compose_prompt","text":"(chr) glue prompts components (chr) complete system prompt (chr) complete user prompt","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Create a prompt to ChatGPT — compose_prompt","text":"compose_sys_prompt(): compose_usr_prompt():","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a prompt to ChatGPT — compose_prompt","text":"","code":"if (FALSE) {   compose_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\",     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",    style = \"Usa un tono amichevole, colloquiale, ma preciso.\"  ) } if (FALSE) {   msg_sys <- compose_sys_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\"  ) }   msg_usr <- compose_usr_prompt(     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",    style = \"Usa un tono amichevole, colloquiale, ma preciso.\"  )"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a prompt to OpenAI API — compose_prompt_api","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"Questa funzione è un semplice wrapper per comporre un prompt per le API OpenAI ChatGPT. Per la sua semplicità, per lo più didattica, non considera alternanze successive di prompt nella chat ma solo l'impostazione iniziale del sistema e il primo messaggio dell'utente.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"","code":"compose_prompt_api(sys_prompt = NULL, usr_prompt = NULL)"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"sys_prompt (chr) messaggio da usare per impostare il sistema usr_prompt (chr) messaggio da usare come richiesta al sistema passata dall'utente","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"(chr) una lista di due lista, la prima con il messaggio da usare per il prompt di impostazione del sistema di assistenza delle API, la seconda con il prompt di richiesta dell'utente.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"genere, una conversazione è formattata con un messaggio di sistema, seguito da messaggi alternati dell'utente e dell'assistente. Il messaggio di sistema consente di impostare il comportamento dell'assistente. Ad esempio, è possibile modificare la personalità dell'assistente o fornire istruzioni specifiche sul comportamento da tenere durante la conversazione. Tuttavia, il messaggio di sistema è facoltativo e il comportamento del modello senza un messaggio di sistema sarà probabilmente simile quello di un messaggio generico come \"Sei un assistente utile\". messaggi dell'utente forniscono richieste o commenti cui l'assistente deve rispondere. messaggi dell'assistente memorizzano le risposte precedenti dell'assistente, ma possono anche essere scritti dall'utente per fornire esempi del comportamento desiderato.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/compose_prompt_api.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"","code":"msg_sys <- compose_sys_prompt(   role = \"Sei l'assistente di un docente universitario.\",   context = \"     Tu e lui state preparando un workshop sull'utilizzo di ChatGPT     per biostatisitci ed epidemiologi.\"  )  msg_usr <- compose_usr_prompt(   task = \"     Il tuo compito è trovare cosa dire per spiegare cosa sia una     chat di ChatGPT agli studenti, considerando che potrebbe     esserci qualcuno che non ne ha mai sentito parlare (e segue     il worksho incuriosito dal titolo o dagli amici).\",   output = \"     Riporta un potenziale dialogo tra il docente e gli studenti     che assolva ed esemplifichi lo scopo descritto.\",  style = \"Usa un tono amichevole, colloquiale, ma preciso.\" )  compose_prompt_api(msg_sys, msg_usr) #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Sei l'assistente di un docente universitario.\\n\\n    Tu e lui state preparando un workshop sull'utilizzo di ChatGPT\\n    per biostatisitci ed epidemiologi.\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"\\n    Il tuo compito è trovare cosa dire per spiegare cosa sia una\\n    chat di ChatGPT agli studenti, considerando che potrebbe\\n    esserci qualcuno che non ne ha mai sentito parlare (e segue\\n    il worksho incuriosito dal titolo o dagli amici).\\n\\n    Riporta un potenziale dialogo tra il docente e gli studenti\\n    che assolva ed esemplifichi lo scopo descritto.\\nUsa un tono amichevole, colloquiale, ma preciso.\" #>  #>"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/create_usr_data_prompter.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a function to prompt the user for data — create_usr_data_prompter","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"function create function can used prompt user data specific context. Given interested context, function created accept string text input return complete prompt based desired context.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/create_usr_data_prompter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"","code":"create_usr_data_prompter(usr_prompt = NULL, delimiter = NULL, closing = NULL)"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/create_usr_data_prompter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"usr_prompt (chr) user prompt use template text added. delimiter (chr) delimiters text embed, sequence four identical symbols suggested. closing (chr) Text include end prompt","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/create_usr_data_prompter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"(function) function can used prompt user, accepting string text input returning complete prompt based desired context.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/create_usr_data_prompter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"","code":"usr_prmpt <- compose_prompt(   role = \"You are the assistant of a university professor.\",   context = \"     You are analyzing the comments of the students of the last course.\",   task = \"Your task is to extract information from a text provided.\",   instructions = \"     You should extract the first and last words of the text.\",   output = \"     Return the first and last words of the text separated by a dash,     i.e., `first - last`.\",   style = \"     Do not add any additional information,     return only the requested information.\",   examples = \"       # Examples:       text: 'This is an example text.'       output: 'This - text'       text: 'Another example text!!!'       output: 'Another - text'\"   ) prompter <- create_usr_data_prompter(   usr_prompt = usr_prmpt ) prompter(\"This is an example text.\") #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nThis is an example text.\\n\\\"\\\"\\\"\" prompter(\"Another example text!!!\") #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nAnother example text!!!\\n\\\"\\\"\\\"\"  # You can also use it with a data frame to programmaically create # prompts for each row of a data frame's column. db <- data.frame(   text = c(\"This is an example text.\", \"Another example text!!!\") ) db$text |> purrr::map_chr(prompter) #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nThis is an example text.\\n\\\"\\\"\\\"\" #> [2] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nAnother example text!!!\\n\\\"\\\"\\\"\""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":null,"dir":"Reference","previous_headings":"","what":"Get completion from chat messages — get_completion_from_messages","title":"Get completion from chat messages — get_completion_from_messages","text":"Get completion chat messages Get content chat completion Get number token chat completion","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get completion from chat messages — get_completion_from_messages","text":"","code":"get_completion_from_messages(   messages,   model = \"gpt-3.5-turbo\",   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   seed = NULL,   use_py = FALSE )  get_content(completion)  get_tokens(completion, what = c(\"total\", \"prompt\", \"completion\", \"all\"))"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get completion from chat messages — get_completion_from_messages","text":"messages (list) following format: ⁠list(list(\"role\" = \"user\", \"content\" = \"Hey! old ?\") (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-model) model (chr, default = \"gpt-3.5-turbo\") length one character vector indicating model use (see: https://platform.openai.com/docs/models/continuous-model-upgrades) temperature (dbl, default = 0) value 0 (deterministic answer) 2 (random). (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature) max_tokens (dbl, default = 500) value greater 0. maximum number tokens generate chat completion. (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens) endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python completion number tokens used output get_completion_from_messages call (chr) one \"total\" (default), \"prompt\", \"completion\", \"\"","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get completion from chat messages — get_completion_from_messages","text":"(list) two element: content, contains chr vector response, tokens, list number tokens used request (prompt_tokens), answer (completion_tokens), overall (total_tokens, sum two) (chr) output message returned assistant (int) number token used completion prompt completion part, overall (total)","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get completion from chat messages — get_completion_from_messages","text":"argument description, please refer official documentation. Lower values temperature result consistent outputs, higher values generate diverse creative results. Select temperature value based desired trade-coherence creativity specific application. Setting temperature 0 make outputs mostly deterministic, small amount variability remain.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Get completion from chat messages — get_completion_from_messages","text":"get_content(): get_tokens():","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/get_completion_from_messages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get completion from chat messages — get_completion_from_messages","text":"","code":"if (FALSE) {   prompt <- list(     list(       role = \"system\",       content = \"you are an assistant who responds succinctly\"     ),     list(       role = \"user\",       content = \"Return the text: 'Hello world'.\"     )   )   res <- get_completion_from_messages(prompt)   answer <- get_content(res) # \"Hello world.\"   token_used <- get_tokens(res) # 30 }  if (FALSE) {   msg_sys <- compose_sys_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\"   )    msg_usr <- compose_usr_prompt(     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",     style = \"Usa un tono amichevole, colloquiale, ma preciso.\"   )    prompt <- compose_prompt_api(msg_sys, msg_usr)   res <- get_completion_from_messages(prompt, \"gpt-4-turbo\")   answer <- get_content(res)   token_used <- get_tokens(res) }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Query the GPT model — query_gpt","title":"Query the GPT model — query_gpt","text":"Query GPT model","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query the GPT model — query_gpt","text":"","code":"query_gpt(   prompt,   model = \"gpt-3.5-turbo\",   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   max_try = 10,   quiet = TRUE,   na_if_error = FALSE,   seed = NULL,   use_py = FALSE )"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query the GPT model — query_gpt","text":"prompt (chr) prompt use model (chr) model use temperature (dbl) temperature use max_tokens (dbl) maximum number tokens endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. max_try (int) maximum number tries quiet (lgl) whether print information na_if_error (lgl) whether return NA error occurs seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python ","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query the GPT model — query_gpt","text":"(list) result query","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query the GPT model — query_gpt","text":"","code":"if (FALSE) {  prompt <- compose_prompt_api(    sys_prompt = compose_sys_prompt(      role = \"Sei l'assistente di un docente universitario.\",      context = \"        Tu e lui state preparando un workshop sull'utilizzo di ChatGPT        per biostatisitci ed epidemiologi.\"    ),    usr_prompt = compose_usr_prompt(      task = \"        Il tuo compito è trovare cosa dire per spiegare cosa sia una        chat di ChatGPT agli studenti, considerando che potrebbe        esserci qualcuno che non ne ha mai sentito parlare (e segue        il worksho incuriosito dal titolo o dagli amici).\",      output = \"        Riporta un potenziale dialogo tra il docente e gli studenti        che assolva ed esemplifichi lo scopo descritto.\",      style = \"Usa un tono amichevole, colloquiale, ma preciso.\"    )  )  res <- query_gpt(prompt)  get_content(res)  get_tokens(res) }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt_on_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Query GPT on a dataframe's column — query_gpt_on_column","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"Query GPT dataframe's column","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt_on_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"","code":"query_gpt_on_column(   db,   text_column,   sys_prompt = NULL,   usr_prompt = NULL,   closing = NULL,   model = \"gpt-3.5-turbo\",   quiet = TRUE,   max_try = 10,   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   add = TRUE,   simplify = TRUE,   na_if_error = FALSE,   res_name = \"gpt_res\",   .progress = TRUE,   seed = NULL,   use_py = FALSE )"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt_on_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"db (data.frame) data use text_column (chr) name column containing text data sys_prompt (chr) system prompt use usr_prompt (chr) user prompt use closing (chr, default = NULL) Text include end prompt model (chr, default = \"gpt-3.5-turbo\") model use quiet (lgl, default = TRUE) whether print information max_try (int, default = 10) maximum number tries temperature (dbl, default = 0) temperature use max_tokens (dbl, default = 1000) maximum number tokens endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. add (lgl, default = TRUE) whether add result original dataframe. FALSE, returns tibble result . simplify (lgl, default = TRUE) whether simplify output na_if_error (lgl, default = FALSE) whether return NA error occurs res_name (chr, default = \"gpt_res\") name column containing result .progress (lgl, default = TRUE) whether show progress bar seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python ","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt_on_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"(tibble) result query","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/query_gpt_on_column.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"","code":"if (FALSE) {   db <- tibble(    commenti = c(      \"Che barba, che noia!\",      \"Un po' noioso, ma interessante\",      \"Che bello, mi è piaciuto molto!\"    )  )   role <- \"Sei l'assistente di un docente universitario.\"  context <- \"State analizzando i commenti degli studenti dell'ultimo corso.\"  task <- \"Il tuo compito è capire se sono soddisfatti del corso.\"  instructions <- \"Analizza i commenti e decidi se sono soddisfatti o meno.\"  output <- \"Riporta 'soddisfatto' o 'insoddisfatto'.\"  style <- \"Non aggiungere nessun commento, restituisci solo ed    esclusivamente la classificazione.\"  examples <- \"  commento_1: 'Mi è piaciuto molto il corso; davvero interessante.'  classificazione_1: 'soddisfatto'  commento_2: 'Non mi è piaciuto per niente; una noia mortale'  classificazione_2: 'insoddisfatto'  \"   sys_prompt <- compose_sys_prompt(role = role, context = context)  usr_prompt <- compose_usr_prompt(    task = task, instructions = instructions, output = output,    style = style, examples = examples  )  res <- db |>   query_gpt_on_column(     \"commenti\", sys_prompt = sys_prompt, usr_prompt = usr_prompt   )  res }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/setup_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup Python environment — setup_py","title":"Setup Python environment — setup_py","text":"function creates virtual environment installs openai package .","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/setup_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup Python environment — setup_py","text":"","code":"setup_py(venv_name = \"r-gpt-venv\", ask = interactive())"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/setup_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup Python environment — setup_py","text":"venv_name (chr, default \"r-gpt-venv\") name virtual environment created. ask (lgl, default TRUE interactive session, FALSE otherwise) TRUE, user asked want create virtual environment.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/setup_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup Python environment — setup_py","text":"(lgl) TRUE virtual environment created, FALSE otherwise.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/setup_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup Python environment — setup_py","text":"","code":"if (FALSE) {   library(ubep.gpt)   setup_py()    prompt <- compose_prompt_api(     sys_prompt = \"You are the assistant of a university professor.\",     usr_prompt = \"Tell me about the last course you provided.\"   )    res <- query_gpt(       prompt = prompt,       use_py = TRUE     ) |>       get_content()     cat(res) }"},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/ubep.gpt-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ubep.gpt: A basic and simple interface to OpenAI’s GPT API — ubep.gpt-package","title":"ubep.gpt: A basic and simple interface to OpenAI’s GPT API — ubep.gpt-package","text":"goal 'ubep.gpt' provide basic simple interface OpenAI's GPT API (compatible APIs). package also designed work (.e., query ) dataframes/tibbles, simplify process querying API.","code":""},{"path":[]},{"path":"https://ubesp-dctv.github.io/ubep.gpt/reference/ubep.gpt-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ubep.gpt: A basic and simple interface to OpenAI’s GPT API — ubep.gpt-package","text":"Maintainer: Corrado Lanera corrado.lanera@ubep.unipd.(ORCID)","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-029","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.9","title":"ubep.gpt 0.2.9","text":"Added setup_py function setup python backend. update README","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-028","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.8","title":"ubep.gpt 0.2.8","text":"Added use_py argument gpt_query, gpt_query_on_column, get_completion_from_messages","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-027","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.7","title":"ubep.gpt 0.2.7","text":"Added closing argument compose_usr_prompt, compose_prompt, create_usr_data_prompter functions, allow add text end prompt, .e., embedded text.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-025","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.5","title":"ubep.gpt 0.2.5","text":"Added seed argument gpt_query, gpt_query_on_column, get_completion_from_messages functions.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-024","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.4","title":"ubep.gpt 0.2.4","text":"Now create_usr_data_prompter can accept custom delimiter. Default delimiter changed four quotes (\"\"\"\") three quotes (\"\"\").","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-023","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.3","title":"ubep.gpt 0.2.3","text":"stream = FALSE hard coded (moment) get_completion_from_messages. column name results gpt_query_on_columns now customizable. Now gpt_query_on_columns returns original tibble column added , including option add return result new tibble single column (add = FALSE). Add progress bar gpt_query_on_column functions.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-022","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.2","title":"ubep.gpt 0.2.2","text":"hot-fix old calls match.arg(model) gpt_query gpt_query_on_column functions.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-021","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.1","title":"ubep.gpt 0.2.1","text":"Add option return NA API returns error; apply gpt_query gpt_query_on_column functions (.e., base get_completion_from_messages function).","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-020","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.2.0","title":"ubep.gpt 0.2.0","text":"Removed dependency openai favor httr jsonlite directly Now queries can made personalized endpoints. create_usr_data_prompter now works empty characters (treated NULL). Now compose_prompt_api correctly manage empty prompts.","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-011","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.1.1","title":"ubep.gpt 0.1.1","text":"100% coverage passed Activated tests CI","code":""},{"path":"https://ubesp-dctv.github.io/ubep.gpt/news/index.html","id":"ubepgpt-010","dir":"Changelog","previous_headings":"","what":"ubep.gpt 0.1.0","title":"ubep.gpt 0.1.0","text":"functions tested. Prompt compositors (.e., compose_prompt, compose_usr_prompt, compose_sys_prompt, create_usr_data_prompter) now always return character vector (possibly length 0). query_gpt_on_column now accepts sys_prompt usr_prompt arguments customize user system prompts. (fix #1) Changed functions names uniform. .e., compose_prompt_user compose_prompt_system now called compose_usr_prompt compose_sys_prompt; usr_msg sys_msg arguments now called usr_prompt sys_prompt. (fix #2) Added zzz.R startup messages checking API keys. Update README examples usage. Setup development environment. Initial setup CorradoLanera/gpt-template.","code":""}]
