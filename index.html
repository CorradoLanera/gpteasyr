<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="The goal of gpteasyr is to provide a basic and simple interface to OpenAI's GPT API (and other compatible APIs). The package is also designed to work with (i.e., to query on) dataframes/tibbles, and to simplify the process of querying the API.">
<title>A basic and simple interface to OpenAI’s GPT API • gpteasyr</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="A basic and simple interface to OpenAI’s GPT API">
<meta property="og:description" content="The goal of gpteasyr is to provide a basic and simple interface to OpenAI's GPT API (and other compatible APIs). The package is also designed to work with (i.e., to query on) dataframes/tibbles, and to simplify the process of querying the API.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="index.html">gpteasyr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/CorradoLanera/gpteasyr/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="gpteasyr">gpteasyr<a class="anchor" aria-label="anchor" href="#gpteasyr"></a>
</h1></div>
<!-- badges: start -->

<p>The goal of <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a> is to provide a basic/simple interface to OpenAI’s GPT API. The package is designed to work with dataframes/tibbles and to simplify the process of querying the API.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install the development version of <a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a> like so:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">remotes</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"CorradoLanera/gpteasyr"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="basic-example">Basic example<a class="anchor" aria-label="anchor" href="#basic-example"></a>
</h2>
<p>You can use the <code>query_gpt</code> function to query the GPT API. You can decide the model to use (e.g., <code>gpt-3.5-turbo</code>, <code>gpt-4-turbo</code>, or <code>gpt-4o</code>). This function is useful because mainly it iterate the query a decided number of times (10 by default) in case of error (often caused by server overload).</p>
<p>To use the function you need to compose a prompt. You can use (but it is not necessary!) the <code>compose_prompt_api</code> function to compose the prompt properly with an optional (single) system prompt (i.e., gpt’s setup) and a (single) user prompt (i.e., the query). This function is useful because it helps you to compose the prompt automatically adopting the required API’s structure.</p>
<blockquote>
<p>NOTE: you can still pass a correctly formatted list (of lists) as described in the <a href="https://platform.openai.com/docs/api-reference/chat" class="external-link">official documentation</a> (<a href="https://platform.openai.com/docs/api-reference/chat" class="external-link uri">https://platform.openai.com/docs/api-reference/chat</a>).</p>
</blockquote>
<p>Once you have queried the API, you can extract the content of the response using the <code>get_content</code> function. You can also extract the tokens of the prompt and the response using the <code>get_tokens</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/CorradoLanera/gpteasyr" class="external-link">gpteasyr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Wellcome to gpteasyr!</span></span>
<span><span class="co">#&gt; The OPENAI_API_KEY environment variable is set</span></span>
<span><span class="co">#&gt; You are ready to use the package `gpteasyr`.</span></span>
<span><span class="co">#&gt; Just, double check if the key is the correct one.</span></span>
<span><span class="co">#&gt; REMIND: Never share your API key with others.</span></span>
<span><span class="co">#&gt;       Keep it safe and secure.</span></span>
<span><span class="co">#&gt;       If you think that your API key was compromised,</span></span>
<span><span class="co">#&gt;       you can regenerate it in the OpenAI-API website</span></span>
<span><span class="co">#&gt;       (https://platform.openai.com/api-keys).</span></span>
<span><span class="co">#&gt; Enjoy the package!</span></span>
<span><span class="co">#&gt; If you like to use the python backend (working only for GPT's OpenAI requests!),</span></span>
<span><span class="co">#&gt; setup the environmen first by executing:</span></span>
<span><span class="co">#&gt; `setup_py()`(default virtual environment name is 'r-gpt-venv').</span></span>
<span><span class="co">#&gt; If you need to change the default name, run:</span></span>
<span><span class="co">#&gt; `setup_py("&lt;your_custom_environment_name&gt;")`</span></span>
<span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span></span>
<span>  sys_prompt <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  usr_prompt <span class="op">=</span> <span class="st">"Tell me about the last course you provided."</span></span>
<span><span class="op">)</span></span>
<span><span class="va">prompt</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; [[1]]$role</span></span>
<span><span class="co">#&gt; [1] "system"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[1]]$content</span></span>
<span><span class="co">#&gt; [1] "You are the assistant of a university professor."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; [[2]]$role</span></span>
<span><span class="co">#&gt; [1] "user"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]$content</span></span>
<span><span class="co">#&gt; [1] "Tell me about the last course you provided."</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>  prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-3.5-turbo"</span>,</span>
<span>  quiet <span class="op">=</span> <span class="cn">FALSE</span>, <span class="co"># default TRUE</span></span>
<span>  max_try <span class="op">=</span> <span class="fl">2</span>, <span class="co"># default 10</span></span>
<span>  temperature <span class="op">=</span> <span class="fl">1.5</span>, <span class="co"># default 0 [0-2]</span></span>
<span>  max_tokens <span class="op">=</span> <span class="fl">100</span> <span class="co"># default the maximum allowed for the selected model</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; ℹ Total tries: 1.</span></span>
<span><span class="co">#&gt; ℹ Prompt token used: 29.</span></span>
<span><span class="co">#&gt; ℹ Response token used: 91.</span></span>
<span><span class="co">#&gt; ℹ Total token used: 120.</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 7</span></span>
<span><span class="co">#&gt;  $ id                : chr "chatcmpl-9RKY0lxKiWocZTzRHJWCtPTcaH2w9"</span></span>
<span><span class="co">#&gt;  $ object            : chr "chat.completion"</span></span>
<span><span class="co">#&gt;  $ created           : int 1716300868</span></span>
<span><span class="co">#&gt;  $ model             : chr "gpt-3.5-turbo-0125"</span></span>
<span><span class="co">#&gt;  $ choices           :'data.frame':  1 obs. of  5 variables:</span></span>
<span><span class="co">#&gt;   ..$ index          : int 0</span></span>
<span><span class="co">#&gt;   ..$ logprobs       : logi NA</span></span>
<span><span class="co">#&gt;   ..$ finish_reason  : chr "stop"</span></span>
<span><span class="co">#&gt;   ..$ message.role   : chr "assistant"</span></span>
<span><span class="co">#&gt;   ..$ message.content: chr "The last course I provided was an undergraduate seminar on \"Advanced Topics in Linguistics.\" The course focus"| __truncated__</span></span>
<span><span class="co">#&gt;  $ usage             :List of 3</span></span>
<span><span class="co">#&gt;   ..$ prompt_tokens    : int 29</span></span>
<span><span class="co">#&gt;   ..$ completion_tokens: int 91</span></span>
<span><span class="co">#&gt;   ..$ total_tokens     : int 120</span></span>
<span><span class="co">#&gt;  $ system_fingerprint: NULL</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "The last course I provided was an undergraduate seminar on \"Advanced Topics in Linguistics.\" The course focused on the contemporary theories and research findings in areas such as syntax, semantics, phonetics, and psycholinguistics. The students were actively engaged in discussing and analyzing research papers, and they also had the opportunity to conduct their own research projects throughout the semester. Overall, it was a stimulating and rewarding experience for both the students and myself as the instructor."</span></span>
<span></span>
<span><span class="co"># for a well formatted output on R, use `cat()`</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; The last course I provided was an undergraduate seminar on "Advanced Topics in Linguistics." The course focused on the contemporary theories and research findings in areas such as syntax, semantics, phonetics, and psycholinguistics. The students were actively engaged in discussing and analyzing research papers, and they also had the opportunity to conduct their own research projects throughout the semester. Overall, it was a stimulating and rewarding experience for both the students and myself as the instructor.</span></span>
<span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 120</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span>, <span class="st">"prompt"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 29</span></span>
<span><span class="fu"><a href="reference/get_completion_from_messages.html">get_tokens</a></span><span class="op">(</span><span class="va">res</span>, <span class="st">"all"</span><span class="op">)</span></span>
<span><span class="co">#&gt;     prompt_tokens completion_tokens      total_tokens </span></span>
<span><span class="co">#&gt;                29                91               120</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="easy-prompt-assisted-creation">Easy prompt-assisted creation<a class="anchor" aria-label="anchor" href="#easy-prompt-assisted-creation"></a>
</h2>
<p>You can use the <code>compose_sys_prompt</code> and <code>compose_usr_prompt</code> functions to create the system and user prompts, respectively. These functions are useful because they help you to compose the prompts following best practices in composing prompt. In fact the arguments are just the main components every good prompt should have. They do just that, composing the prompt for you juxtaposing the components in order.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sys_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_sys_prompt</a></span><span class="op">(</span></span>
<span>  role <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  context <span class="op">=</span> <span class="st">"You are analyzing the comments of the students of the last course."</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">sys_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; You are the assistant of a university professor.</span></span>
<span><span class="co">#&gt; You are analyzing the comments of the students of the last course.</span></span>
<span></span>
<span><span class="va">usr_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_usr_prompt</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="st">"Your task is to extract information from a text provided."</span>,</span>
<span>  instructions <span class="op">=</span> <span class="st">"You should extract the first and last words of the text."</span>,</span>
<span>  output <span class="op">=</span> <span class="st">"Return the first and last words of the text separated by a dash, i.e., `first - last`."</span>,</span>
<span>  style <span class="op">=</span> <span class="st">"Do not add any additional information, return only the requested information."</span>,</span>
<span>  examples <span class="op">=</span> <span class="st">"</span></span>
<span><span class="st">    # Examples:</span></span>
<span><span class="st">    text: 'This is an example text.'</span></span>
<span><span class="st">    output: 'This - text'</span></span>
<span><span class="st">    text: 'Another example text!!!'</span></span>
<span><span class="st">    output: 'Another - text'"</span>,</span>
<span>  text <span class="op">=</span> <span class="st">"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura"</span>,</span>
<span>  closing <span class="op">=</span> <span class="st">"Take a deep breath and work on the problem step-by-step."</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">usr_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; Your task is to extract information from a text provided.</span></span>
<span><span class="co">#&gt; You should extract the first and last words of the text.</span></span>
<span><span class="co">#&gt; Return the first and last words of the text separated by a dash, i.e., `first - last`.</span></span>
<span><span class="co">#&gt; Do not add any additional information, return only the requested information.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     # Examples:</span></span>
<span><span class="co">#&gt;     text: 'This is an example text.'</span></span>
<span><span class="co">#&gt;     output: 'This - text'</span></span>
<span><span class="co">#&gt;     text: 'Another example text!!!'</span></span>
<span><span class="co">#&gt;     output: 'Another - text'</span></span>
<span><span class="co">#&gt; """</span></span>
<span><span class="co">#&gt; Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura</span></span>
<span><span class="co">#&gt; """</span></span>
<span><span class="co">#&gt; Take a deep breath and work on the problem step-by-step.</span></span>
<span></span>
<span><span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span><span class="va">sys_prompt</span>, <span class="va">usr_prompt</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Nel - step-by-step"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="querying-a-column-of-a-dataframe">Querying a column of a dataframe<a class="anchor" aria-label="anchor" href="#querying-a-column-of-a-dataframe"></a>
</h2>
<p>You can use the <code>query_gpt_on_column</code> function to query the GPT API on a column of a dataframe. This function is useful because it helps you to iterate the query on each row of the column and to compose the prompt automatically adopting the required API’s structure. In this case, you need to provide the components of the prompt creating the prompt template, and the name of the column you what to embed in the template as a “text” to query. All the prompt’s components are optional, so you can provide only the ones you need: <code>role</code> and <code>context</code> compose the system prompt, while <code>task</code>, <code>instructions</code>, <code>output</code>, <code>style</code>, and <code>examples</code> compose the user prompt (they will be just juxtaposed in the right order)</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">db</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  txt <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"I'm very satisfied with the course; it was very interesting and useful."</span>,</span>
<span>    <span class="st">"I didn't like it at all; it was deadly boring."</span>,</span>
<span>    <span class="st">"The best course I've ever attended."</span>,</span>
<span>    <span class="st">"The course was a waste of time."</span>,</span>
<span>    <span class="st">"blah blah blah"</span>,</span>
<span>    <span class="st">"woow"</span>,</span>
<span>    <span class="st">"bim bum bam"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># system</span></span>
<span><span class="va">role</span> <span class="op">&lt;-</span> <span class="st">"You are the assistant of a university professor."</span></span>
<span><span class="va">context</span> <span class="op">&lt;-</span> <span class="st">"You are analyzing the comments of the students of the last course."</span></span>
<span></span>
<span><span class="co"># user</span></span>
<span><span class="va">task</span> <span class="op">&lt;-</span> <span class="st">"Your task is to understand if they are satisfied with the course."</span></span>
<span><span class="va">instructions</span> <span class="op">&lt;-</span> <span class="st">"Analyze the comments and decide if they are satisfied or not."</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="st">"Report 'satisfied' or 'unsatisfied', in case of doubt or impossibility report 'NA'."</span></span>
<span><span class="va">style</span> <span class="op">&lt;-</span> <span class="st">"Do not add any comment, return only and exclusively one of the possible classifications."</span></span>
<span></span>
<span><span class="va">examples</span> <span class="op">&lt;-</span> <span class="st">"</span></span>
<span><span class="st">  # Examples:</span></span>
<span><span class="st">  text: 'I'm very satisfied with the course; it was very interesting and useful.'</span></span>
<span><span class="st">  output: 'satisfied'</span></span>
<span><span class="st">  text: 'I didn't like it at all; it was deadly boring.'</span></span>
<span><span class="st">  output: 'unsatisfied'"</span></span>
<span></span>
<span><span class="va">closing</span> <span class="op">&lt;-</span> <span class="st">"Take a deep breath and work on the problem step-by-step."</span> <span class="co"># This will be added AFTER the embedded text</span></span>
<span></span>
<span><span class="va">sys_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_sys_prompt</a></span><span class="op">(</span>role <span class="op">=</span> <span class="va">role</span>, context <span class="op">=</span> <span class="va">context</span><span class="op">)</span></span>
<span><span class="va">usr_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_usr_prompt</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="va">task</span>,</span>
<span>  instructions <span class="op">=</span> <span class="va">instructions</span>,</span>
<span>  output <span class="op">=</span> <span class="va">output</span>,</span>
<span>  style <span class="op">=</span> <span class="va">style</span>,</span>
<span>  examples <span class="op">=</span> <span class="va">examples</span></span>
<span>  <span class="co"># don't put the `closing` here if you want to use it on</span></span>
<span>  <span class="co"># `query_gpt_on_column` after the embedded text;</span></span>
<span>  <span class="co"># if here, it will go after the examples but before the embedded text.</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">db</span> <span class="op">|&gt;</span></span>
<span> <span class="fu"><a href="reference/query_gpt_on_column.html">query_gpt_on_column</a></span><span class="op">(</span></span>
<span>   text_column <span class="op">=</span> <span class="st">"txt"</span>,  <span class="co"># the name of the column containing the text to</span></span>
<span>                         <span class="co"># analyze after being embedded in the prompt.</span></span>
<span>   sys_prompt <span class="op">=</span> <span class="va">sys_prompt</span>,</span>
<span>   usr_prompt <span class="op">=</span> <span class="va">usr_prompt</span>,</span>
<span>   closing <span class="op">=</span> <span class="va">closing</span>,  <span class="co"># this will be added AFTER the embedded text</span></span>
<span>   na_if_error <span class="op">=</span> <span class="cn">TRUE</span>,  <span class="co"># dafault is FALSE, and in case of error the</span></span>
<span>                        <span class="co"># the error will be signaled and computation </span></span>
<span>                        <span class="co"># stopped.</span></span>
<span>   .progress <span class="op">=</span> <span class="cn">FALSE</span>  <span class="co"># default is TRUE, and progress bar will be shown.</span></span>
<span> <span class="op">)</span></span>
<span><span class="co">#&gt;                                                                       txt</span></span>
<span><span class="co">#&gt; 1 I'm very satisfied with the course; it was very interesting and useful.</span></span>
<span><span class="co">#&gt; 2                          I didn't like it at all; it was deadly boring.</span></span>
<span><span class="co">#&gt; 3                                     The best course I've ever attended.</span></span>
<span><span class="co">#&gt; 4                                         The course was a waste of time.</span></span>
<span><span class="co">#&gt; 5                                                          blah blah blah</span></span>
<span><span class="co">#&gt; 6                                                                    woow</span></span>
<span><span class="co">#&gt; 7                                                             bim bum bam</span></span>
<span><span class="co">#&gt;       gpt_res</span></span>
<span><span class="co">#&gt; 1   satisfied</span></span>
<span><span class="co">#&gt; 2 unsatisfied</span></span>
<span><span class="co">#&gt; 3   satisfied</span></span>
<span><span class="co">#&gt; 4 unsatisfied</span></span>
<span><span class="co">#&gt; 5        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 6        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 7        &lt;NA&gt;</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="robust-example-with-for-loops-and-error-handling">Robust example with for loops and error handling<a class="anchor" aria-label="anchor" href="#robust-example-with-for-loops-and-error-handling"></a>
</h2>
<p>This example is useful for long computation in which errors from the server-side can happened (maybe after days of querying). The following script will save each result one-by one, so that in case of error the evaluated results won’t be lost.</p>
<p>In case of any error, the error message(s) will be reported as a warning, but it does not stop the computation. Moreover, re-executing the loop will evaluate the queries only where they were failed or not performed yet.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># This is a function that take a text and attach it at the end of the</span></span>
<span><span class="co"># original provided prompt</span></span>
<span></span>
<span><span class="co"># install.packages("depigner")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://corradolanera.github.io/depigner/" class="external-link">depigner</a></span><span class="op">)</span> <span class="co"># for progress bar `pb_len()` and `tick()`</span></span>
<span><span class="co">#&gt; Welcome to depigner: we are here to un-stress you!</span></span>
<span><span class="va">usr_prompter</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/create_usr_data_prompter.html">create_usr_data_prompter</a></span><span class="op">(</span><span class="va">usr_prompt</span>, closing <span class="op">=</span> <span class="va">closing</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">db</span><span class="op">)</span></span>
<span><span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA_character_</span></span>
<span></span>
<span><span class="va">pb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://corradolanera.github.io/depigner/reference/pb_len.html" class="external-link">pb_len</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu">checkmate</span><span class="fu">::</span><span class="fu"><a href="https://mllg.github.io/checkmate/reference/checkScalarNA.html" class="external-link">test_scalar_na</a></span><span class="op">(</span><span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">db</span><span class="op">[[</span><span class="st">"gpt_res"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>      prompt <span class="op">=</span> <span class="fu"><a href="reference/compose_prompt_api.html">compose_prompt_api</a></span><span class="op">(</span></span>
<span>        sys_prompt <span class="op">=</span> <span class="va">sys_prompt</span>,</span>
<span>        usr_prompt <span class="op">=</span> <span class="fu">usr_prompter</span><span class="op">(</span><span class="va">db</span><span class="op">[[</span><span class="st">"txt"</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>      <span class="op">)</span>,</span>
<span>      na_if_error <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>      <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="fu"><a href="https://corradolanera.github.io/depigner/reference/pb_len.html" class="external-link">tick</a></span><span class="op">(</span><span class="va">pb</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Row"</span>, <span class="va">i</span>, <span class="st">"of"</span>, <span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; evaluated: Row 6 of 7 [========================&gt;----]  86% in  2s [ETA:  0s]evaluated: Row 7 of 7 [=============================] 100% in  3s [ETA:  0s]</span></span>
<span></span>
<span><span class="va">db</span></span>
<span><span class="co">#&gt;                                                                       txt</span></span>
<span><span class="co">#&gt; 1 I'm very satisfied with the course; it was very interesting and useful.</span></span>
<span><span class="co">#&gt; 2                          I didn't like it at all; it was deadly boring.</span></span>
<span><span class="co">#&gt; 3                                     The best course I've ever attended.</span></span>
<span><span class="co">#&gt; 4                                         The course was a waste of time.</span></span>
<span><span class="co">#&gt; 5                                                          blah blah blah</span></span>
<span><span class="co">#&gt; 6                                                                    woow</span></span>
<span><span class="co">#&gt; 7                                                             bim bum bam</span></span>
<span><span class="co">#&gt;       gpt_res</span></span>
<span><span class="co">#&gt; 1   satisfied</span></span>
<span><span class="co">#&gt; 2 unsatisfied</span></span>
<span><span class="co">#&gt; 3   satisfied</span></span>
<span><span class="co">#&gt; 4 unsatisfied</span></span>
<span><span class="co">#&gt; 5        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 6        &lt;NA&gt;</span></span>
<span><span class="co">#&gt; 7        &lt;NA&gt;</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="base-chatgpt-prompt-creation-not-for-api">Base ChatGPT prompt creation (NOT for API)<a class="anchor" aria-label="anchor" href="#base-chatgpt-prompt-creation-not-for-api"></a>
</h2>
<p>You can use the <code>compose_prompt</code> function to create a prompt for ChatGPT. This function is useful because it helps you to compose the prompt following best practices in composing prompt. In fact the arguments are just the main components every good prompt should have. They do just that, composing the prompt for you juxtaposing the components in the right order.</p>
<blockquote>
<p>WARNING: The result is suitable to be copy-pasted on ChatGPT, not to be used with API calls, i.e., it cannot be used with the <code>query_gpt</code> function!</p>
</blockquote>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">chat_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/compose_prompt.html">compose_prompt</a></span><span class="op">(</span></span>
<span>  role <span class="op">=</span> <span class="st">"You are the assistant of a university professor."</span>,</span>
<span>  context <span class="op">=</span> <span class="st">"You are analyzing the comments of the students of the last course."</span>,</span>
<span>  task <span class="op">=</span> <span class="st">"Your task is to extract information from a text provided."</span>,</span>
<span>  instructions <span class="op">=</span> <span class="st">"You should extract the first and last words of the text."</span>,</span>
<span>  output <span class="op">=</span> <span class="st">"Return the first and last words of the text separated by a dash, i.e., `first - last`."</span>,</span>
<span>  style <span class="op">=</span> <span class="st">"Do not add any additional information, return only the requested information."</span>,</span>
<span>  examples <span class="op">=</span> <span class="st">"</span></span>
<span><span class="st">    # Examples:</span></span>
<span><span class="st">    text: 'This is an example text.'</span></span>
<span><span class="st">    output: 'This - text'</span></span>
<span><span class="st">    text: 'Another example text!!!'</span></span>
<span><span class="st">    output: 'Another - text'"</span>,</span>
<span>  text <span class="op">=</span> <span class="st">"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">chat_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; You are the assistant of a university professor.</span></span>
<span><span class="co">#&gt; You are analyzing the comments of the students of the last course.</span></span>
<span><span class="co">#&gt; Your task is to extract information from a text provided.</span></span>
<span><span class="co">#&gt; You should extract the first and last words of the text.</span></span>
<span><span class="co">#&gt; Return the first and last words of the text separated by a dash, i.e., `first - last`.</span></span>
<span><span class="co">#&gt; Do not add any additional information, return only the requested information.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     # Examples:</span></span>
<span><span class="co">#&gt;     text: 'This is an example text.'</span></span>
<span><span class="co">#&gt;     output: 'This - text'</span></span>
<span><span class="co">#&gt;     text: 'Another example text!!!'</span></span>
<span><span class="co">#&gt;     output: 'Another - text'</span></span>
<span><span class="co">#&gt; """"</span></span>
<span><span class="co">#&gt; Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura</span></span>
<span><span class="co">#&gt; """"</span></span></code></pre></div>
<figure><img src="dev/img/gpt-example.png" alt="https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d"><figcaption aria-hidden="true"><a href="https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d" class="external-link uri">https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d</a>
</figcaption></figure>
</div>
<div class="section level2">
<h2 id="other-options-and-utilities">Other options and utilities<a class="anchor" aria-label="anchor" href="#other-options-and-utilities"></a>
</h2>
<div class="section level3">
<h3 id="options-for-temperature-max_tokens-and-seed">Options for <code>temperature</code>, <code>max_tokens</code>, and <code>seed</code>
<a class="anchor" aria-label="anchor" href="#options-for-temperature-max_tokens-and-seed"></a>
</h3>
<p>You cannot use all the option of official APIs (<a href="https://platform.openai.com/docs/api-reference/chat/create" class="external-link uri">https://platform.openai.com/docs/api-reference/chat/create</a>), we select the following to be available here (please contact the authors if you need more):</p>
<ul>
<li>
<code>temperature</code>: “What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.”</li>
<li>
<code>max_tokens</code>: “The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length.”</li>
<li>
<code>seed</code>, “This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.”</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    temperature <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>    max_tokens <span class="op">=</span> <span class="fl">30</span>,</span>
<span>    seed <span class="op">=</span> <span class="fl">1234</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span> <span class="co"># limited to 30 tokens!</span></span>
<span><span class="co">#&gt; The last course my professor provided was a graduate-level seminar on cutting-edge research topics in environmental science. The course covered a range of interdisciplinary subjects related to</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="pythons-backend">Python’s backend<a class="anchor" aria-label="anchor" href="#pythons-backend"></a>
</h3>
<p>Often, for complex prompt it happens that the R environment (everyone we have experimented, i.e. <a href="https://github.com/irudnyts/openai" class="external-link">openai</a>, <a href="https://httr.r-lib.org/" class="external-link">httr</a>, <a href="https://httr2.r-lib.org" class="external-link">httr2</a>, and <code>curl</code>) return a timeout error for a certificate validation (see, e.g.: <a href="https://github.com/irudnyts/openai/issues/61" class="external-link uri">https://github.com/irudnyts/openai/issues/61</a>, and <a href="https://github.com/irudnyts/openai/issues/42" class="external-link uri">https://github.com/irudnyts/openai/issues/42</a>). The same does not happen with a pure python backend using the official OpenAI’s <a href="https://github.com/irudnyts/openai" class="external-link">openai</a> library. you can setup a Python backend by executing <code><a href="reference/setup_py.html">setup_py()</a></code>, and setting <code>use_py = TRUE</code> in the functions that send the queries (i.e., <code>query_gpt</code>, <code>query_gpt_on_column</code>, and <code>get_completion_from_messages</code>)</p>
<blockquote>
<p>NOTE: using a Python backend can be a little slower, but sometimes necessary.</p>
</blockquote>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/setup_py.html">setup_py</a></span><span class="op">(</span>ask <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; virtualenv: r-gpt-venv</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    use_py <span class="op">=</span> <span class="cn">TRUE</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; The last course I provided was an advanced seminar on environmental sustainability in urban planning. The course covered topics such as green infrastructure, sustainable transportation, and climate change adaptation strategies in urban areas. Students engaged in discussions, group projects, and case studies to explore real-world applications of sustainable urban planning principles. Overall, it was a very engaging and informative course that challenged students to think critically about the intersection of environmental sustainability and urban development.</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="personalized-servers-endpoint">Personalized server’s endpoint<a class="anchor" aria-label="anchor" href="#personalized-servers-endpoint"></a>
</h3>
<p>If you have a personal server asking for queries using the OpenAI’s API format, (e.g. using LM Studio, with open source models), you can set the endpoint to POST the query on your server instead of the OpenaAI one.</p>
<blockquote>
<p>NOTE: when using personalized server endpoint, you can select the model you would like to use as usual by the <code>model</code> option. Clearly, available models depend on your local server configuration.</p>
</blockquote>
<blockquote>
<p>WARNING: this option cannot be select if Python backend is request (i.e., setting <code>use_py = TRUE</code>, and a custom <code>endpoint</code> won’t work)!</p>
</blockquote>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># we do not run this in the README</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/query_gpt.html">query_gpt</a></span><span class="op">(</span></span>
<span>    prompt <span class="op">=</span> <span class="va">prompt</span>,</span>
<span>    endopont <span class="op">=</span> <span class="st">"http://localhost:1234/v1/chat/completions"</span>,</span>
<span>    model <span class="op">=</span> <span class="st">"lmstudio-ai/gemma-2b-it-GGUF"</span></span>
<span> <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="reference/get_completion_from_messages.html">get_content</a></span><span class="op">(</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="code-of-conduct">Code of Conduct<a class="anchor" aria-label="anchor" href="#code-of-conduct"></a>
</h2>
<p>Please note that the gpteasyr project is released with a <a href="https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html" class="external-link">Contributor Code of Conduct</a>. By contributing to this project, you agree to abide by its terms.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/CorradoLanera/gpteasyr/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/CorradoLanera/gpteasyr/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>

<div class="community">
<h2 data-toc-skip>Community</h2>
<ul class="list-unstyled">
<li><a href="CODE_OF_CONDUCT.html">Code of conduct</a></li>
</ul>
</div>

<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing gpteasyr</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Corrado Lanera <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0002-0520-7428" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a></li>
<li><a href="https://app.codecov.io/gh/CorradoLanera/gpteasyr?branch=main" class="external-link"><img src="https://codecov.io/gh/CorradoLanera/gpteasyr/branch/main/graph/badge.svg" alt="Codecov test coverage"></a></li>
<li><a href="https://github.com/CorradoLanera/gpteasyr/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/CorradoLanera/gpteasyr/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Corrado Lanera.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
