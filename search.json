[{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement Corrado.Lanera@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 CorradoLanera Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Corrado Lanera. Author, maintainer.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lanera C (2024). gpteasyr: basic simple interface OpenAI’s GPT API. R package version 0.5.0, https://CorradoLanera.github.io/gpteasyr/, https://github.com/CorradoLanera/gpteasyr.","code":"@Manual{,   title = {gpteasyr: A basic and simple interface to OpenAI’s GPT API},   author = {Corrado Lanera},   year = {2024},   note = {R package version 0.5.0,     https://CorradoLanera.github.io/gpteasyr/},   url = {https://github.com/CorradoLanera/gpteasyr}, }"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"gpteasyr","dir":"","previous_headings":"","what":"A basic and simple interface to OpenAI’s GPT API","title":"A basic and simple interface to OpenAI’s GPT API","text":"goal gpteasyr provide basic/simple interface OpenAI’s GPT API. package designed work dataframes/tibbles simplify process querying API.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A basic and simple interface to OpenAI’s GPT API","text":"can install development version gpteasyr like :","code":"remotes::install_github(\"CorradoLanera/gpteasyr\")"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic example","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use query_gpt function query GPT API. can decide model use (e.g., gpt-3.5-turbo, gpt-4o, gpt-4o-mini). function useful mainly iterate query decided number times (10 default) case error (often caused server overload). use function, need compose prompt. can use (necessary!) compose_prompt_api function compose prompt properly optional (single) system prompt (.e., gpt’s setup) (single) user prompt (.e., query). function useful helps compose prompt automatically adopting required API’s structure. NOTE: can still pass multiple fully-formatted list (lists) described official documentation (https://platform.openai.com/docs/api-reference/chat). queried API, can extract content response using get_content function. can also extract tokens prompt response using get_tokens function.","code":"library(gpteasyr) #> Wellcome to `{gpteasyr}`! #> The OPENAI_API_KEY environment variable is set #> You are ready to use the package `{gpteasyr}`. #> Just, double check if the key is the correct one. #> REMIND: Never share your API key with others. #>       Keep it safe and secure. #>       If you think that your API key was compromised, #>       you can regenerate it in the OpenAI-API website #>       (https://platform.openai.com/api-keys), or contacting your GPT's admin. #>  #> Enjoy GPT with `{gpteasyr}`! prompt <- compose_prompt_api(   sys_prompt = \"You are the assistant of a university professor.\",   usr_prompt = \"Tell me about the last course you provided.\" ) prompt #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"You are the assistant of a university professor.\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"Tell me about the last course you provided.\"  res <- query_gpt(   prompt = prompt,   model = \"gpt-4o-mini\",   quiet = FALSE, # default TRUE   max_try = 2, # default 10   temperature = 1.5, # default 0 [0-2]   max_tokens = 100 # default the maximum allowed for the selected model ) #> ℹ Total tries: 1. #> ℹ Prompt token used: 29. #> ℹ Response token used: 64. #> ℹ Total token used: 93.  str(res) #> List of 7 #>  $ id                : chr \"chatcmpl-9mletd4NzxIN41yj1lKc7UCyiIS0F\" #>  $ object            : chr \"chat.completion\" #>  $ created           : int 1721409971 #>  $ model             : chr \"gpt-4o-mini-2024-07-18\" #>  $ choices           :'data.frame':  1 obs. of  4 variables: #>   ..$ index        : int 0 #>   ..$ message      :'data.frame':    1 obs. of  2 variables: #>   .. ..$ role   : chr \"assistant\" #>   .. ..$ content: chr \"As an AI, I don't personally teach courses or run specific academic logs. However, I can help you outline what \"| __truncated__ #>   ..$ logprobs     : logi NA #>   ..$ finish_reason: chr \"stop\" #>  $ usage             :List of 3 #>   ..$ prompt_tokens    : int 29 #>   ..$ completion_tokens: int 64 #>   ..$ total_tokens     : int 93 #>  $ system_fingerprint: chr \"fp_8b761cb050\" get_content(res) #> [1] \"As an AI, I don't personally teach courses or run specific academic logs. However, I can help you outline what an educational course might cover! Please provide details on the subject you'd like to focus on, and I can assist in describing the content, structure, assignments, or learning objectives for a course in that subject.\"  # for a well formatted output on R, use `cat()` get_content(res) |> cat() #> As an AI, I don't personally teach courses or run specific academic logs. However, I can help you outline what an educational course might cover! Please provide details on the subject you'd like to focus on, and I can assist in describing the content, structure, assignments, or learning objectives for a course in that subject.  get_tokens(res) # default is \"total\" #> [1] 93 get_tokens(res, \"prompt\") # \"total\", \"prompt\", \"completion\" (i.e., the answer) #> [1] 29 get_tokens(res, \"all\") #>     prompt_tokens completion_tokens      total_tokens  #>                29                64                93"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"easy-prompt-assisted-creation","dir":"","previous_headings":"","what":"Easy prompt-assisted creation","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use compose_sys_prompt compose_usr_prompt functions create system user prompts, respectively. functions useful help compose prompts following best practices composing prompt. fact arguments just main components every good prompt . just , composing prompt juxtaposing components order.","code":"sys_prompt <- compose_sys_prompt(   role = \"You are the assistant of a university professor.\",   context = \"You are analyzing the comments of the students of the last course.\" ) cat(sys_prompt) #> You are the assistant of a university professor. #> You are analyzing the comments of the students of the last course.  usr_prompt <- compose_usr_prompt(   task = \"Your task is to extract information from a text provided.\",   instructions = \"You should extract the first and last words of the text.\",   output = \"Return the first and last words of the text separated by a dash, i.e., `first - last`.\",   style = \"Do not add any additional information, return only the requested information.\",   examples = \"     # Examples:     text: 'This is an example text.'     output: 'This - text'     text: 'Another example text!!!'     output: 'Another - text'\",   text = \"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\",   closing = \"Take a deep breath and work on the problem step-by-step.\" ) cat(usr_prompt) #> Your task is to extract information from a text provided. #> You should extract the first and last words of the text. #> Return the first and last words of the text separated by a dash, i.e., `first - last`. #> Do not add any additional information, return only the requested information. #>  #>     # Examples: #>     text: 'This is an example text.' #>     output: 'This - text' #>     text: 'Another example text!!!' #>     output: 'Another - text' #> \"\"\" #> Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura #> \"\"\" #> Take a deep breath and work on the problem step-by-step.  compose_prompt_api(sys_prompt, usr_prompt) |>    query_gpt() |>    get_content() #> [1] \"Nel - oscura\""},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"querying-a-column-of-a-dataframe","dir":"","previous_headings":"","what":"Querying a column of a dataframe","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use query_gpt_on_column function query GPT API column dataframe. function useful helps iterate query row column compose prompt automatically adopting required API’s structure. case, need provide components prompt creating prompt template, name column embed template “text” query. prompt’s components optional, can provide ones need: role context compose system prompt, task, instructions, output, style, examples compose user prompt (just juxtaposed right order)","code":"db <- data.frame(   txt = c(     \"I'm very satisfied with the course; it was very interesting and useful.\",     \"I didn't like it at all; it was deadly boring.\",     \"The best course I've ever attended.\",     \"The course was a waste of time.\",     \"blah blah blah\",     \"woow\",     \"bim bum bam\"   ) )  # system role <- \"You are the assistant of a university professor.\" context <- \"You are analyzing the comments of the students of the last course.\"  # user task <- \"Your task is to understand if they are satisfied with the course.\" instructions <- \"Analyze the comments and decide if they are satisfied or not.\" output <- \"Report 'satisfied' or 'unsatisfied', in case of doubt or impossibility report 'NA'.\" style <- \"Do not add any comment, return only and exclusively one of the possible classifications.\"  examples <- \"   # Examples:   text: 'I'm very satisfied with the course; it was very interesting and useful.'   output: 'satisfied'   text: 'I didn't like it at all; it was deadly boring.'   output: 'unsatisfied'\"  closing <- \"Take a deep breath and work on the problem step-by-step.\" # This will be added AFTER the embedded text  sys_prompt <- compose_sys_prompt(role = role, context = context) usr_prompt <- compose_usr_prompt(   task = task,   instructions = instructions,   output = output,   style = style,   examples = examples   # If you want to put a `closing` after the text embedded by the use of   # `query_gpt_on_column`, you usually shouldn't include it here as   # well: if put here, it will go after the examples but before the text   # embedded by `query_gpt_on_column`; In borderline cases, you might   # still free to decide to put it here, or even both. )  db |>  query_gpt_on_column(    text_column = \"txt\",  # the name of the column containing the text to                          # analyze after being embedded in the prompt.    sys_prompt = sys_prompt,    usr_prompt = usr_prompt,    closing = closing,  # this will be added AFTER the embedded text    na_if_error = TRUE,  # dafault is FALSE, and in case of error the                         # the error will be signaled and computation                          # stopped.    .progress = FALSE  # default is TRUE, and progress bar will be shown.  ) #>                                                                       txt #> 1 I'm very satisfied with the course; it was very interesting and useful. #> 2                          I didn't like it at all; it was deadly boring. #> 3                                     The best course I've ever attended. #> 4                                         The course was a waste of time. #> 5                                                          blah blah blah #> 6                                                                    woow #> 7                                                             bim bum bam #>       gpt_res #> 1   satisfied #> 2 unsatisfied #> 3   satisfied #> 4 unsatisfied #> 5        <NA> #> 6        <NA> #> 7        <NA>"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"robust-example-with-for-loops-and-error-handling","dir":"","previous_headings":"","what":"Robust example with for loops and error handling","title":"A basic and simple interface to OpenAI’s GPT API","text":"example useful long computation errors server-side can happened (maybe days querying). following script save result one-one, case error evaluated results won’t lost. case error, error message(s) reported warning, stop computation. Moreover, re-executing loop evaluate queries failed performed yet. NOTE: Object stored (disk) still lost session crashes! maximum robustness, efficiency, security, suggested transpose logic onto targets pipeline (see manual ; idea map record branch single target, successful query never re-executed.)","code":"# This is a function that take a text and attach it at the end of the # original provided prompt  # install.packages(\"depigner\") library(depigner) # for progress bar `pb_len()` and `tick()` #> Welcome to depigner: we are here to un-stress you! usr_prompter <- create_usr_data_prompter(usr_prompt, closing = closing)  n <- nrow(db) db[[\"gpt_res\"]] <- NA_character_  pb <- pb_len(n) for (i in seq_len(n)) {   if (checkmate::test_scalar_na(db[[\"gpt_res\"]][[i]])) {     db[[\"gpt_res\"]][[i]] <- query_gpt(       prompt = compose_prompt_api(         sys_prompt = sys_prompt,         usr_prompt = usr_prompter(db[[\"txt\"]][[i]])       ),       na_if_error = TRUE     ) |>        get_content()   }   tick(pb, paste(\"Row\", i, \"of\", n)) } #>  #> evaluated: Row 4 of 7 [================>------------]  57% in  2s [ETA:  2s]evaluated: Row 5 of 7 [====================>--------]  71% in  3s [ETA:  1s]evaluated: Row 6 of 7 [========================>----]  86% in  3s [ETA:  1s]evaluated: Row 7 of 7 [=============================] 100% in  4s [ETA:  0s]  db #>                                                                       txt #> 1 I'm very satisfied with the course; it was very interesting and useful. #> 2                          I didn't like it at all; it was deadly boring. #> 3                                     The best course I've ever attended. #> 4                                         The course was a waste of time. #> 5                                                          blah blah blah #> 6                                                                    woow #> 7                                                             bim bum bam #>       gpt_res #> 1   satisfied #> 2 unsatisfied #> 3   satisfied #> 4 unsatisfied #> 5        <NA> #> 6        <NA> #> 7        <NA>"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"base-chatgpt-prompt-creation-not-for-api","dir":"","previous_headings":"","what":"Base ChatGPT prompt creation (NOT for API)","title":"A basic and simple interface to OpenAI’s GPT API","text":"can use compose_prompt function create prompt ChatGPT. function useful helps compose prompt following best practices composing prompt. fact arguments just main components every good prompt . just , composing prompt juxtaposing components right order. WARNING: result suitable copy-pasted ChatGPT, used API calls, .e., used query_gpt function! https://chat.openai.com/share/394a008b-d463-42dc-9361-1bd745bcad6d","code":"chat_prompt <- compose_prompt(   role = \"You are the assistant of a university professor.\",   context = \"You are analyzing the comments of the students of the last course.\",   task = \"Your task is to extract information from a text provided.\",   instructions = \"You should extract the first and last words of the text.\",   output = \"Return the first and last words of the text separated by a dash, i.e., `first - last`.\",   style = \"Do not add any additional information, return only the requested information.\",   examples = \"     # Examples:     text: 'This is an example text.'     output: 'This - text'     text: 'Another example text!!!'     output: 'Another - text'\",   text = \"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\" )  cat(chat_prompt) #> You are the assistant of a university professor. #> You are analyzing the comments of the students of the last course. #> Your task is to extract information from a text provided. #> You should extract the first and last words of the text. #> Return the first and last words of the text separated by a dash, i.e., `first - last`. #> Do not add any additional information, return only the requested information. #>  #>     # Examples: #>     text: 'This is an example text.' #>     output: 'This - text' #>     text: 'Another example text!!!' #>     output: 'Another - text' #> \"\"\"\" #> Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura #> \"\"\"\""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"options-for-temperature-max_tokens-and-seed","dir":"","previous_headings":"Other options and utilities","what":"Options for temperature, max_tokens, and seed","title":"A basic and simple interface to OpenAI’s GPT API","text":"use features official APIs (https://platform.openai.com/docs/api-reference/chat/create), selected following available keep interface opinionated balance ease use, efficiency flexibility (please contact authors need ): temperature: “sampling temperature use, 0 2. Higher values like 0.8 make output random, lower values like 0.2 make focused deterministic.” max_tokens: “maximum number tokens can generated chat completion. total length input tokens generated tokens limited model’s context length.” seed, “feature Beta. specified, system make best effort sample deterministically, repeated requests seed parameters return result. Determinism guaranteed, refer system_fingerprint response parameter monitor changes backend.”","code":"res <- query_gpt(     prompt = prompt,     temperature = 1.2,     max_tokens = 30,     seed = 1234  ) |>    get_content()   cat(res) # limited to 30 tokens! #> As an AI, I'm not able to provide specific information about personal experiences or past actions since I don't have the capability to conduct courses or retain personal memories"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"pythons-backend","dir":"","previous_headings":"Other options and utilities","what":"Python’s backend","title":"A basic and simple interface to OpenAI’s GPT API","text":"Often, complex prompt happens R environment (everyone experimented, .e. openai, httr, httr2, curl) return timeout error certificate validation (see, e.g.: https://github.com/irudnyts/openai/issues/61, https://github.com/irudnyts/openai/issues/42). happen pure python backend using official OpenAI’s openai library. can setup Python backend executing setup_py(), setting use_py = TRUE functions send queries (.e., query_gpt, query_gpt_on_column, get_completion_from_messages) NOTE: using Python backend can little slower, sometimes necessary.","code":"setup_py(ask = FALSE) # default TRUE will always ask for confirmation. #> virtualenv: r-gpt-venv  res <- query_gpt(     prompt = prompt,     use_py = TRUE  ) |>    get_content()   cat(res) #> As an AI, I don't have personal experiences or the ability to teach courses myself. However, I can help you design a course, provide information on course content, or assist with any specific topics you might be interested in. If you have a particular subject in mind, please let me know, and I can provide relevant information or resources!"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"personalized-servers-endpoint","dir":"","previous_headings":"Other options and utilities","what":"Personalized server’s endpoint","title":"A basic and simple interface to OpenAI’s GPT API","text":"personal server listens queries using OpenAI API format, (e.g. using LM Studio, open source models), can set endpoint POST query server instead OpenAI one. NOTE: using personalised server endpoint, can select model whish use usual way, .e., using model option. course, available models can select depend local server configuration. WARNING: option selected Python backend requested (.e., setting use_py = TRUE custom endpoint won’t work)!","code":"if (FALSE) { # we do not run this in the README   res <- query_gpt(     prompt = prompt,     endopont = \"http://localhost:1234/v1/chat/completions\",     model = \"lmstudio-ai/gemma-2b-it-GGUF\"  ) |>    get_content()   cat(res) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"batch-openai-requests","dir":"","previous_headings":"","what":"Batch OpenAI requests","title":"A basic and simple interface to OpenAI’s GPT API","text":"April 23, 2024, OpenAI introduced new feature allows send multiple requests single call (see: https://openai.com/index/-enterprise-grade-features--api-customers/). feature now available gpteasyr. can use batch_* functions send multiple requests single call. functions file_upload,","code":"# Create a list of prompts sys_prompt <- compose_sys_prompt(\"You are a funny assistant.\") usr_prompt <- compose_usr_prompt(   \"Tell me a joke ending in:\" ) prompter <- create_usr_data_prompter(usr_prompt = usr_prompt) text <-  c(     \"deadly boring!\",     \"A bit boring, but interesting\",     \"How nice, I loved it!\"   )  prompts <- text |>   purrr::map(     \\(x) compose_prompt_api(       sys_prompt = sys_prompt,       usr_prompt = prompter(x)     )   )  # Create a jsonl file as required by the API, and save it jsonl_text <- create_jsonl_records(prompts) out_jsonl_path <- write_jsonl_files(jsonl_text, tempdir())  # upload the jsonl file to OpenAI project # The project used is the one linked with the API key you have set in # the environment variable `OPENAI_API_KEY` batch_file_info <- file_upload(out_jsonl_path) batch_file_info #> # A tibble: 1 × 8 #>   object id              purpose filename bytes created_at status status_details #>   <chr>  <chr>           <chr>   <chr>    <int>      <int> <chr>  <lgl>          #> 1 file   file-WD0v9Zf1h… batch   2024071…   847 1721409987 proce… NA  # Create a batch job from the id of an uploaded jsonl file batch_job_info <- batch_create(batch_file_info[[\"id\"]]) batch_job_info #> # A tibble: 1 × 22 #>   id               object endpoint errors input_file_id completion_window status #>   <chr>            <chr>  <chr>    <lgl>  <chr>         <chr>             <chr>  #> 1 batch_j2ZEBIayM… batch  /v1/cha… NA     file-WD0v9Zf… 24h               valid… #> # ℹ 15 more variables: output_file_id <lgl>, error_file_id <lgl>, #> #   created_at <int>, in_progress_at <lgl>, expires_at <int>, #> #   finalizing_at <lgl>, completed_at <lgl>, failed_at <lgl>, expired_at <lgl>, #> #   cancelling_at <lgl>, cancelled_at <lgl>, request_counts_total <int>, #> #   request_counts_completed <int>, request_counts_failed <int>, metadata <lgl>  # You can retrieve the status of the batch job by its ID batch_status <- batch_status(batch_job_info[[\"id\"]]) batch_status #> # A tibble: 1 × 22 #>   id               object endpoint errors input_file_id completion_window status #>   <chr>            <chr>  <chr>    <lgl>  <chr>         <chr>             <chr>  #> 1 batch_j2ZEBIayM… batch  /v1/cha… NA     file-WD0v9Zf… 24h               in_pr… #> # ℹ 15 more variables: output_file_id <lgl>, error_file_id <lgl>, #> #   created_at <int>, in_progress_at <int>, expires_at <int>, #> #   finalizing_at <lgl>, completed_at <lgl>, failed_at <lgl>, expired_at <lgl>, #> #   cancelling_at <lgl>, cancelled_at <lgl>, request_counts_total <int>, #> #   request_counts_completed <int>, request_counts_failed <int>, metadata <lgl>  # You can list all the batches in the project (default limit is 10) list_of_batches <- batch_list() list_of_batches #> # A tibble: 10 × 5 #>    object data$id            $object $endpoint $errors first_id last_id has_more #>    <chr>  <chr>              <chr>   <chr>     <lgl>   <chr>    <chr>   <lgl>    #>  1 list   batch_j2ZEBIayM6a… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  2 list   batch_6tE6zWTFOO5… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  3 list   batch_2AHPswv8VDU… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  4 list   batch_XhIO1qXvIgE… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  5 list   batch_MllBz1653SD… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  6 list   batch_ntJoJiyBldj… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  7 list   batch_AkeprQpqku3… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  8 list   batch_uLg2nQXcOLy… batch   /v1/chat… NA      batch_j… batch_… TRUE     #>  9 list   batch_Gl288anB838… batch   /v1/chat… NA      batch_j… batch_… TRUE     #> 10 list   batch_Z5oXvVMQeFS… batch   /v1/chat… NA      batch_j… batch_… TRUE     #> # ℹ 16 more variables: data$input_file_id <chr>, $completion_window <chr>, #> #   $status <chr>, $output_file_id <chr>, $error_file_id <chr>, #> #   $created_at <int>, $in_progress_at <int>, $expires_at <int>, #> #   $finalizing_at <lgl>, $completed_at <lgl>, $failed_at <lgl>, #> #   $expired_at <lgl>, $cancelling_at <int>, $cancelled_at <int>, #> #   $request_counts <df[,3]>, $metadata <lgl>  while (batch_status[[\"status\"]] != \"completed\") {   Sys.sleep(60)   batch_status <- batch_status(batch_job_info[[\"id\"]])   cat(\"Waiting for the batch to be completed...\\n\") } #> Waiting for the batch to be completed...  # Once the batch is completed, you can retrieve the results by results <- batch_result(batch_status[[\"id\"]]) str(results, 2) #> List of 3 #>  $ :List of 7 #>   ..$ id                : chr \"chatcmpl-9mlfAmX2Z0L96gTgll5MyVj9ln6kb\" #>   ..$ object            : chr \"chat.completion\" #>   ..$ created           : int 1721409988 #>   ..$ model             : chr \"gpt-4o-mini-2024-07-18\" #>   ..$ choices           :'data.frame':   1 obs. of  4 variables: #>   ..$ usage             :List of 3 #>   ..$ system_fingerprint: chr \"fp_661538dc1f\" #>  $ :List of 7 #>   ..$ id                : chr \"chatcmpl-9mlfAC9GeibmX0Av13L0mJHdSfZn6\" #>   ..$ object            : chr \"chat.completion\" #>   ..$ created           : int 1721409988 #>   ..$ model             : chr \"gpt-4o-mini-2024-07-18\" #>   ..$ choices           :'data.frame':   1 obs. of  4 variables: #>   ..$ usage             :List of 3 #>   ..$ system_fingerprint: chr \"fp_8b761cb050\" #>  $ :List of 7 #>   ..$ id                : chr \"chatcmpl-9mlfWrGqtbPMjPrELxAWfwS2PeMJX\" #>   ..$ object            : chr \"chat.completion\" #>   ..$ created           : int 1721410010 #>   ..$ model             : chr \"gpt-4o-mini-2024-07-18\" #>   ..$ choices           :'data.frame':   1 obs. of  4 variables: #>   ..$ usage             :List of 3 #>   ..$ system_fingerprint: chr \"fp_8b761cb050\"  # By default the results are simplified to the response body returning  # a list of responses, so you can continue to work as usual. If you want # to have the full response, you can set `simplify = FALSE` in the # `batch_result` call. res <- purrr::map_chr(results, get_content) res #> [1] \"Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field... but his speeches were just deadly boring!\"                 #> [2] \"Why did the mathematician break up with the statistician?\\n\\nBecause every time they went out, it was always a bit boring, but interesting!\" #> [3] \"Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!\\n\\nHow nice, I loved it!\"  # You can cancel a batch job by its ID (if it isn't completed yet) if (FALSE) { # the batch is completed now so this would raise an error   batch_cancelled <- batch_cancel(batch_job_info[[\"id\"]])   batch_cancelled }"},{"path":"https://CorradoLanera.github.io/gpteasyr/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"A basic and simple interface to OpenAI’s GPT API","text":"Please note gpteasyr project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel batch job — batch_cancel","title":"Cancel batch job — batch_cancel","text":"Cancel batch job","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel batch job — batch_cancel","text":"","code":"batch_cancel(batch_id)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel batch job — batch_cancel","text":"batch_id (chr) batch id cancel","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cancel batch job — batch_cancel","text":"(tibble) information cancelled batch","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel batch job — batch_cancel","text":"information, see OpenAI API documentation.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_cancel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cancel batch job — batch_cancel","text":"","code":"if (FALSE) {   batch_cancel(\"batch_abc123\")    batch_file_info <- file_upload(\"abc123.jsonl\")   batch_job_info <- batch_file_info[[\"id\"]] |>     batch_create()   batch_cancelled <- batch_job_info[[\"id\"]] |>     batch_cancel()   batch_cancelled }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create batch — batch_create","title":"Create batch — batch_create","text":"Create batch","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create batch — batch_create","text":"","code":"batch_create(input_file_id)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create batch — batch_create","text":"input_file_id (chr) id input file","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create batch — batch_create","text":"(tibble) information created batch","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create batch — batch_create","text":"information, see OpenAI API documentation.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_create.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create batch — batch_create","text":"","code":"if (FALSE) {   batch_file_info <- file_upload(\"abc123.jsonl\")   batch_job_info <- batch_file_info[[\"id\"]] |>     batch_create()   batch_job_info }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List all batches — batch_list","title":"List all batches — batch_list","text":"List batches","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all batches — batch_list","text":"","code":"batch_list(n = 10)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all batches — batch_list","text":"n (int) number batches retrieve","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all batches — batch_list","text":"(tibble) information batches","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List all batches — batch_list","text":"information, see OpenAI API documentation.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List all batches — batch_list","text":"","code":"if (FALSE) {   batch_list()   batch_list(2)   batch_list(Inf) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve batch results — batch_result","title":"Retrieve batch results — batch_result","text":"Retrieve batch results","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve batch results — batch_result","text":"","code":"batch_result(batch_id, simplify = TRUE)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve batch results — batch_result","text":"batch_id (chr) batch id retrieve simplify (lgl, default TRUE) whether simplify output, .e. return response body single standard completions (default), full response.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve batch results — batch_result","text":"(list) simplify TRUE, list batch results; otherwise, list full responses.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve batch results — batch_result","text":"information, see OpenAI API documentation.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_result.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve batch results — batch_result","text":"","code":"if (FALSE) {   batch_file_info <- file_upload(\"abc123.jsonl\")   batch_job_info <- batch_file_info[[\"id\"]] |>     batch_create()   batch_status <- batch_job_info[[\"id\"]] |>     batch_status()    # once the batch is completed   if (batch_status[[\"status\"]] == \"completed\") {     results <- batch_status[[\"id\"]] |>       batch_result()     res <- results |>       purrr::map_chr(get_content)     res   }    full_results <- batch_status[[\"id\"]] |>     batch_result(simplify = FALSE)   str(full_results, 2)   full_res <- full_results |>     purrr::map_chr(\\(x) get_content(x[[\"response\"]][[\"body\"]]))   full_res    identical(res, full_res) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve batch status — batch_status","title":"Retrieve batch status — batch_status","text":"function retrieves status batch.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve batch status — batch_status","text":"","code":"batch_status(batch_id = \"\")"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve batch status — batch_status","text":"batch_id (chr) batch id check","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve batch status — batch_status","text":"(tibble) information batch status","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve batch status — batch_status","text":"information, see OpenAI API documentation. information, see OpenAI API documentation.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/batch_status.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve batch status — batch_status","text":"","code":"if (FALSE) {   batch_status(\"batch_abc123\")    batch_file_info <- file_upload(\"abc123.jsonl\")   batch_job_info <- batch_file_info[[\"id\"]] |>     batch_create()   batch_status <- batch_job_info[[\"id\"]] |>     batch_status()   batch_status }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a prompt to ChatGPT — compose_prompt","title":"Create a prompt to ChatGPT — compose_prompt","text":"function simple wrapper compose good prompt ChatGPT. output nothing juxtaposition separate lines various components (additional text enclosed delimiters bottom prompt). use focused useful remembering getting used entering components useful good prompt.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a prompt to ChatGPT — compose_prompt","text":"","code":"compose_prompt(   role = NULL,   context = NULL,   task = NULL,   instructions = NULL,   output = NULL,   style = NULL,   examples = NULL,   text = NULL,   closing = NULL,   delimiter = if (is.null(text)) NULL else \"\\\"\\\"\\\"\\\"\" )  compose_sys_prompt(role = NULL, context = NULL)  compose_usr_prompt(   task = NULL,   instructions = NULL,   output = NULL,   style = NULL,   examples = NULL,   text = NULL,   closing = NULL,   delimiter = if (is.null(text)) NULL else \"\\\"\\\"\\\"\" )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a prompt to ChatGPT — compose_prompt","text":"role (chr) role ChatGPT play context (chr) context behind task required task (chr) tasks ChatGPT assess instructions (chr) Description steps ChatGPT follow output (chr) type/kind output required style (chr) style ChatGPT use output examples (chr) examples correct output text (chr) Additional text embed prompt closing (chr) Text include end prompt delimiter (chr) delimiters text embed, sequence three identical symbols suggested","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a prompt to ChatGPT — compose_prompt","text":"(chr) glue prompts components (chr) complete system prompt (chr) complete user prompt","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Create a prompt to ChatGPT — compose_prompt","text":"compose_sys_prompt(): compose_usr_prompt():","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a prompt to ChatGPT — compose_prompt","text":"","code":"if (FALSE) {   compose_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\",     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",    style = \"Usa un tono amichevole, colloquiale, ma preciso.\"  ) } if (FALSE) {   msg_sys <- compose_sys_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\"  ) }   msg_usr <- compose_usr_prompt(     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",    style = \"Usa un tono amichevole, colloquiale, ma preciso.\"  )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a prompt to OpenAI API — compose_prompt_api","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"Questa funzione è un semplice wrapper per comporre un prompt per le API OpenAI ChatGPT. Per la sua semplicità, per lo più didattica, non considera alternanze successive di prompt nella chat ma solo l'impostazione iniziale del sistema e il primo messaggio dell'utente.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"","code":"compose_prompt_api(sys_prompt = NULL, usr_prompt = NULL)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"sys_prompt (chr) messaggio da usare per impostare il sistema usr_prompt (chr) messaggio da usare come richiesta al sistema passata dall'utente","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"(chr) una lista di due lista, la prima con il messaggio da usare per il prompt di impostazione del sistema di assistenza delle API, la seconda con il prompt di richiesta dell'utente.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"genere, una conversazione è formattata con un messaggio di sistema, seguito da messaggi alternati dell'utente e dell'assistente. Il messaggio di sistema consente di impostare il comportamento dell'assistente. Ad esempio, è possibile modificare la personalità dell'assistente o fornire istruzioni specifiche sul comportamento da tenere durante la conversazione. Tuttavia, il messaggio di sistema è facoltativo e il comportamento del modello senza un messaggio di sistema sarà probabilmente simile quello di un messaggio generico come \"Sei un assistente utile\". messaggi dell'utente forniscono richieste o commenti cui l'assistente deve rispondere. messaggi dell'assistente memorizzano le risposte precedenti dell'assistente, ma possono anche essere scritti dall'utente per fornire esempi del comportamento desiderato.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/compose_prompt_api.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a prompt to OpenAI API — compose_prompt_api","text":"","code":"msg_sys <- compose_sys_prompt(   role = \"Sei l'assistente di un docente universitario.\",   context = \"     Tu e lui state preparando un workshop sull'utilizzo di ChatGPT     per biostatisitci ed epidemiologi.\"  )  msg_usr <- compose_usr_prompt(   task = \"     Il tuo compito è trovare cosa dire per spiegare cosa sia una     chat di ChatGPT agli studenti, considerando che potrebbe     esserci qualcuno che non ne ha mai sentito parlare (e segue     il worksho incuriosito dal titolo o dagli amici).\",   output = \"     Riporta un potenziale dialogo tra il docente e gli studenti     che assolva ed esemplifichi lo scopo descritto.\",  style = \"Usa un tono amichevole, colloquiale, ma preciso.\" )  compose_prompt_api(msg_sys, msg_usr) #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Sei l'assistente di un docente universitario.\\n\\n    Tu e lui state preparando un workshop sull'utilizzo di ChatGPT\\n    per biostatisitci ed epidemiologi.\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"\\n    Il tuo compito è trovare cosa dire per spiegare cosa sia una\\n    chat di ChatGPT agli studenti, considerando che potrebbe\\n    esserci qualcuno che non ne ha mai sentito parlare (e segue\\n    il worksho incuriosito dal titolo o dagli amici).\\n\\n    Riporta un potenziale dialogo tra il docente e gli studenti\\n    che assolva ed esemplifichi lo scopo descritto.\\nUsa un tono amichevole, colloquiale, ma preciso.\" #>  #>"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_jsonl_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Create jsonl records — create_jsonl_records","title":"Create jsonl records — create_jsonl_records","text":"function creates jsonl file tibble.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_jsonl_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create jsonl records — create_jsonl_records","text":"","code":"create_jsonl_records(   prompt,   id = seq_along(prompt),   model = \"gpt-4o-mini\",   temperature = 0,   max_tokens = NULL,   seed = NULL,   prefix = \"request-\" )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_jsonl_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create jsonl records — create_jsonl_records","text":"prompt (list) messages included jsonl record. id (int) id record. model (chr, default = \"gpt-4o-mini\") model used. temperature (dbl) temperature use max_tokens (dbl) maximum number tokens seed (dbl) seed use prefix (chr, default = \"request-\") prefix custom id.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_jsonl_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create jsonl records — create_jsonl_records","text":"(chr) jsonl records.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_jsonl_records.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create jsonl records — create_jsonl_records","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union   library(purrr)   library(stringr)   library(gpteasyr)    db <- tibble(     commenti = c(       \"deadly boring!\",       \"A bit boring, but interesting\",       \"How nice, I loved it!\"     )   )    role <- \"Sei l'assistente di un docente universitario.\"   context <- \"State analizzando i commenti degli studenti dell'ultimo corso.\"   task <- \"Il tuo compito è capire se sono soddisfatti del corso.\"   instructions <- \"Analizza i commenti e decidi se sono soddisfatti o meno.\"   output <- \"Riporta 'soddisfatto' o 'insoddisfatto'.\"   style <- \"Non aggiungere nessun commento, restituisci solo ed     esclusivamente la classificazione.\"   examples <- \"   commento_1: 'Mi è piaciuto molto il corso; davvero interessante.'   classificazione_1: 'soddisfatto'   commento_2: 'Non mi è piaciuto per niente; una noia mortale'   classificazione_2: 'insoddisfatto'   \"    sys_prompt <- compose_sys_prompt(role = role, context = context)   usr_prompt <- compose_usr_prompt(     task = task, instructions = instructions, output = output,     style = style, examples = examples   )    prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)    res <- db |>     mutate(       id = row_number(),       prompt = commenti |>       map(         \\(x) compose_prompt_api(           sys_prompt = sys_prompt,           usr_prompt = prompter(x)         )       )     )    jsonl_direct <- create_jsonl_records(res[[\"prompt\"]], res[[\"id\"]]) |>     str_c(collapse = \"\\n\")    jsonl_on_db <- res |>     mutate(       jsonl = create_jsonl_records(prompt, id)     ) |>     pull(jsonl) |>     str_c(collapse = \"\\n\")    identical(jsonl_on_db, jsonl_direct) #> [1] TRUE"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_usr_data_prompter.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a function to prompt the user for data — create_usr_data_prompter","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"function create function can used prompt user data specific context. Given interested context, function created accept string text input return complete prompt based desired context.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_usr_data_prompter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"","code":"create_usr_data_prompter(usr_prompt = NULL, delimiter = NULL, closing = NULL)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_usr_data_prompter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"usr_prompt (chr) user prompt use template text added. delimiter (chr) delimiters text embed, sequence four identical symbols suggested. closing (chr) Text include end prompt","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_usr_data_prompter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"(function) function can used prompt user, accepting string text input returning complete prompt based desired context.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/create_usr_data_prompter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a function to prompt the user for data — create_usr_data_prompter","text":"","code":"usr_prompt <- compose_prompt(   role = \"You are the assistant of a university professor.\",   context = \"     You are analyzing the comments of the students of the last course.\",   task = \"Your task is to extract information from a text provided.\",   instructions = \"     You should extract the first and last words of the text.\",   output = \"     Return the first and last words of the text separated by a dash,     i.e., `first - last`.\",   style = \"     Do not add any additional information,     return only the requested information.\",   examples = \"       # Examples:       text: 'This is an example text.'       output: 'This - text'       text: 'Another example text!!!'       output: 'Another - text'\"   ) prompter <- create_usr_data_prompter(   usr_prompt = usr_prompt ) prompter(\"This is an example text.\") #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nThis is an example text.\\n\\\"\\\"\\\"\" prompter(\"Another example text!!!\") #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nAnother example text!!!\\n\\\"\\\"\\\"\"  # You can also use it with a data frame to programmatically create # prompts for each row of a data frame's column. db <- data.frame(   text = c(\"This is an example text.\", \"Another example text!!!\") ) db$text |> purrr::map_chr(prompter) #> [1] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nThis is an example text.\\n\\\"\\\"\\\"\" #> [2] \"You are the assistant of a university professor.\\n\\n    You are analyzing the comments of the students of the last course.\\nYour task is to extract information from a text provided.\\n\\n    You should extract the first and last words of the text.\\n\\n    Return the first and last words of the text separated by a dash,\\n    i.e., `first - last`.\\n\\n    Do not add any additional information,\\n    return only the requested information.\\n\\n      # Examples:\\n      text: 'This is an example text.'\\n      output: 'This - text'\\n      text: 'Another example text!!!'\\n      output: 'Another - text'\\n\\\"\\\"\\\"\\nAnother example text!!!\\n\\\"\\\"\\\"\""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete file — file_delete","title":"Delete file — file_delete","text":"Delete file","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete file — file_delete","text":"","code":"file_delete(file_id)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete file — file_delete","text":"file_id (chr) id file delete.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_delete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete file — file_delete","text":"(tibble) information deleted file","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_delete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delete file — file_delete","text":"","code":"sys_prompt <- compose_sys_prompt(\"You are a funny assistant.\")  usr_prompt <- compose_usr_prompt(    \"Tell me a joke ending in:\"  )  prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)  text <-  c(    \"deadly boring!\",    \"A bit boring, but interesting\",    \"How nice, I loved it!\"  )   jsonl_text <- text |>    purrr::map(      \\(x) {        compose_prompt_api(          sys_prompt = sys_prompt,          usr_prompt = prompter(x)        )      }    ) |>    create_jsonl_records()  out_jsonl_path <- write_jsonl_files(jsonl_text, tempdir())  uploaded_file_info <- file_upload(out_jsonl_path) #> Error: API request failed [401]: #>  #> Invalid authorization header file_delete(uploaded_file_info[[\"id\"]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'uploaded_file_info' not found"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve file information — file_info","title":"Retrieve file information — file_info","text":"Retrieve file information","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve file information — file_info","text":"","code":"file_info(file_id)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve file information — file_info","text":"file_id (chr) id file retrieve.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve file information — file_info","text":"(tibble) information file","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve file information — file_info","text":"","code":"sys_prompt <- compose_sys_prompt(\"You are a funny assistant.\")  usr_prompt <- compose_usr_prompt(    \"Tell me a joke ending in:\"  )  prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)  text <-  c(    \"deadly boring!\",    \"A bit boring, but interesting\",    \"How nice, I loved it!\"  )   jsonl_text <- text |>    purrr::map(      \\(x) {        compose_prompt_api(          sys_prompt = sys_prompt,          usr_prompt = prompter(x)        )      }    ) |>    create_jsonl_records()  out_jsonl_path <- write_jsonl_files(jsonl_text, tempdir())  uploaded_file_info <- file_upload(out_jsonl_path) #> Error: API request failed [401]: #>  #> Invalid authorization header files <- file_list() #> Error: API request failed [401]: #>  #> Invalid authorization header file_info(files[[\"id\"]][[1]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'files' not found file_delete(uploaded_file_info[[\"id\"]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'uploaded_file_info' not found"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List uploaded file — file_list","title":"List uploaded file — file_list","text":"List uploaded file","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List uploaded file — file_list","text":"","code":"file_list(purpose = NULL)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List uploaded file — file_list","text":"purpose (chr) return files given purpose. Can omitted (.e., NULL), \"batch\" \"batch_output\".","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List uploaded file — file_list","text":"(tibble) information uploaded files","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List uploaded file — file_list","text":"","code":"file_list() #> Error: API request failed [401]: #>  #> Invalid authorization header file_list(\"batch\") #> Error: API request failed [401]: #>  #> Invalid authorization header file_list(\"batch_output\") #> Error: API request failed [401]: #>  #> Invalid authorization header"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_retrieve.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve file content — file_retrieve","title":"Retrieve file content — file_retrieve","text":"Retrieve file content","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_retrieve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve file content — file_retrieve","text":"","code":"file_retrieve(file_id, convert_json = FALSE)"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_retrieve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve file content — file_retrieve","text":"file_id (chr) id file retrieve. convert_json (lgl) TRUE, convert response JSON.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_retrieve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve file content — file_retrieve","text":"(chr) content file.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_retrieve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve file content — file_retrieve","text":"","code":"sys_prompt <- compose_sys_prompt(\"You are a funny assistant.\")  usr_prompt <- compose_usr_prompt(\"Tell me a joke ending in:\")  prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)  text <-  c(    \"deadly boring!\",    \"A bit boring, but interesting\",    \"How nice, I loved it!\"  )   jsonl_text <- text |>    purrr::map(      \\(x) {        compose_prompt_api(          sys_prompt = sys_prompt,          usr_prompt = prompter(x)        )      }    ) |>    create_jsonl_records()  out_jsonl_path <- write_jsonl_files(jsonl_text, tempdir())  uploaded_file_info <- file_upload(out_jsonl_path) #> Error: API request failed [401]: #>  #> Invalid authorization header file_content <- file_retrieve(uploaded_file_info[[\"id\"]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'uploaded_file_info' not found file_delete(uploaded_file_info[[\"id\"]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'uploaded_file_info' not found"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload batch file — file_upload","title":"Upload batch file — file_upload","text":"Upload batch file","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload batch file — file_upload","text":"","code":"file_upload(jsonl_path, purpose = \"batch\")"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload batch file — file_upload","text":"jsonl_path (chr) path file upload purpose (chr, default = \"batch\") intended purpose uploaded file. Use \"assistants\" Assistants Message files, \"vision\" Assistants image file inputs, \"batch\" Batch API, \"fine-tune\" Fine-tuning.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload batch file — file_upload","text":"(tibble) information uploaded file","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Upload batch file — file_upload","text":"information, see batch documentation. file documentation. moment, 'gpteasyr' tested file upload batch API (still work also).","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"files-for-batch-api-your-input-file-must-be-formatted-as-a","dir":"Reference","previous_headings":"","what":"Files for batch API Your input file must be formatted as a","title":"Upload batch file — file_upload","text":"JSONL file. file can contain 50,000 requests, can 100 MB size, size files uploaded one organization can 100 GB.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/file_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload batch file — file_upload","text":"","code":"sys_prompt <- compose_sys_prompt(\"You are a funny assistant.\")  usr_prompt <- compose_usr_prompt(    \"Tell me a joke ending in:\"  )  prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)  text <-  c(    \"deadly boring!\",    \"A bit boring, but interesting\",    \"How nice, I loved it!\"  )   jsonl_text <- text |>    purrr::map(      \\(x) {        compose_prompt_api(          sys_prompt = sys_prompt,          usr_prompt = prompter(x)        )      }    ) |>    create_jsonl_records()  out_jsonl_path <- write_jsonl_files(jsonl_text, tempdir())  uploaded_file_info <- file_upload(out_jsonl_path) #> Error: API request failed [401]: #>  #> Invalid authorization header file_delete(uploaded_file_info[[\"id\"]]) #> Error: Failed to evaluate glue component {file_id} #> Caused by error: #> ! object 'uploaded_file_info' not found"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":null,"dir":"Reference","previous_headings":"","what":"Get completion from chat messages — get_completion_from_messages","title":"Get completion from chat messages — get_completion_from_messages","text":"Get completion chat messages Get content chat completion Get number token chat completion","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get completion from chat messages — get_completion_from_messages","text":"","code":"get_completion_from_messages(   messages,   model = \"gpt-4o-mini\",   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   seed = NULL,   use_py = FALSE )  get_content(completion)  get_tokens(completion, what = c(\"total\", \"prompt\", \"completion\", \"all\"))"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get completion from chat messages — get_completion_from_messages","text":"messages (list) following format: ⁠list(list(\"role\" = \"user\", \"content\" = \"Hey! old ?\") (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-model) model (chr, default = \"gpt-4o-mini\") length one character vector indicating model use (see: https://platform.openai.com/docs/models/continuous-model-upgrades) temperature (dbl, default = 0) value 0 (deterministic answer) 2 (random). (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature) max_tokens (dbl, default = 500) value greater 0. maximum number tokens generate chat completion. (see: https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens) endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python completion number tokens used output get_completion_from_messages call (chr) one \"total\" (default), \"prompt\", \"completion\", \"\"","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get completion from chat messages — get_completion_from_messages","text":"(list) two element: content, contains chr vector response, tokens, list number tokens used request (prompt_tokens), answer (completion_tokens), overall (total_tokens, sum two) (chr) output message returned assistant (int) number token used completion prompt completion part, overall (total)","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get completion from chat messages — get_completion_from_messages","text":"argument description, please refer official documentation. Lower values temperature result consistent outputs, higher values generate diverse creative results. Select temperature value based desired trade-coherence creativity specific application. Setting temperature 0 make outputs mostly deterministic, small amount variability remain.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Get completion from chat messages — get_completion_from_messages","text":"get_content(): get_tokens():","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/get_completion_from_messages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get completion from chat messages — get_completion_from_messages","text":"","code":"if (FALSE) {   prompt <- list(     list(       role = \"system\",       content = \"you are an assistant who responds succinctly\"     ),     list(       role = \"user\",       content = \"Return the text: 'Hello world'.\"     )   )   res <- get_completion_from_messages(prompt)   answer <- get_content(res) # \"Hello world.\"   token_used <- get_tokens(res) # 30 }  if (FALSE) {   msg_sys <- compose_sys_prompt(     role = \"Sei l'assistente di un docente universitario.\",     context = \"       Tu e lui state preparando un workshop sull'utilizzo di ChatGPT       per biostatisitci ed epidemiologi.\"   )    msg_usr <- compose_usr_prompt(     task = \"       Il tuo compito è trovare cosa dire per spiegare cosa sia una       chat di ChatGPT agli studenti, considerando che potrebbe       esserci qualcuno che non ne ha mai sentito parlare (e segue       il worksho incuriosito dal titolo o dagli amici).\",     output = \"       Riporta un potenziale dialogo tra il docente e gli studenti       che assolva ed esemplifichi lo scopo descritto.\",     style = \"Usa un tono amichevole, colloquiale, ma preciso.\"   )    prompt <- compose_prompt_api(msg_sys, msg_usr)   res <- get_completion_from_messages(prompt, \"gpt-4-turbo\")   answer <- get_content(res)   token_used <- get_tokens(res) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/gpteasyr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"gpteasyr: A basic and simple interface to OpenAI’s GPT API — gpteasyr-package","title":"gpteasyr: A basic and simple interface to OpenAI’s GPT API — gpteasyr-package","text":"goal 'gpteasyr' provide basic simple interface OpenAI's GPT API (compatible APIs). package also designed work (.e., query ) dataframes/tibbles, simplify process querying API.","code":""},{"path":[]},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/gpteasyr-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"gpteasyr: A basic and simple interface to OpenAI’s GPT API — gpteasyr-package","text":"Maintainer: Corrado Lanera corrado.lanera@gmail.com (ORCID)","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Query the GPT model — query_gpt","title":"Query the GPT model — query_gpt","text":"Query GPT model","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query the GPT model — query_gpt","text":"","code":"query_gpt(   prompt,   model = \"gpt-4o-mini\",   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   max_try = 10,   quiet = TRUE,   na_if_error = FALSE,   seed = NULL,   use_py = FALSE )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query the GPT model — query_gpt","text":"prompt (chr) prompt use model (chr) model use temperature (dbl) temperature use max_tokens (dbl) maximum number tokens endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. max_try (int) maximum number tries quiet (lgl) whether print information na_if_error (lgl) whether return NA error occurs seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python ","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query the GPT model — query_gpt","text":"(list) result query","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query the GPT model — query_gpt","text":"","code":"if (FALSE) {  prompt <- compose_prompt_api(    sys_prompt = compose_sys_prompt(      role = \"Sei l'assistente di un docente universitario.\",      context = \"        Tu e lui state preparando un workshop sull'utilizzo di ChatGPT        per biostatisitci ed epidemiologi.\"    ),    usr_prompt = compose_usr_prompt(      task = \"        Il tuo compito è trovare cosa dire per spiegare cosa sia una        chat di ChatGPT agli studenti, considerando che potrebbe        esserci qualcuno che non ne ha mai sentito parlare (e segue        il worksho incuriosito dal titolo o dagli amici).\",      output = \"        Riporta un potenziale dialogo tra il docente e gli studenti        che assolva ed esemplifichi lo scopo descritto.\",      style = \"Usa un tono amichevole, colloquiale, ma preciso.\"    )  )  res <- query_gpt(prompt)  get_content(res)  get_tokens(res) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt_on_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Query GPT on a dataframe's column — query_gpt_on_column","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"Query GPT dataframe's column","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt_on_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"","code":"query_gpt_on_column(   db,   text_column,   sys_prompt = NULL,   usr_prompt = NULL,   closing = NULL,   model = \"gpt-4o-mini\",   quiet = TRUE,   max_try = 10,   temperature = 0,   max_tokens = NULL,   endpoint = \"https://api.openai.com/v1/chat/completions\",   add = TRUE,   simplify = TRUE,   na_if_error = FALSE,   res_name = \"gpt_res\",   .progress = TRUE,   seed = NULL,   use_py = FALSE )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt_on_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"db (data.frame) data use text_column (chr) name column containing text data sys_prompt (chr) system prompt use usr_prompt (chr) user prompt use closing (chr, default = NULL) Text include end prompt model (chr, default = \"gpt-4o-mini\") model use quiet (lgl, default = TRUE) whether print information max_try (int, default = 10) maximum number tries temperature (dbl, default = 0) temperature use max_tokens (dbl, default = 1000) maximum number tokens endpoint (chr, default = \"https://api.openai.com/v1/chat/completions\", .e. OpenAI API) endpoint use request. add (lgl, default = TRUE) whether add result original dataframe. FALSE, returns tibble result . simplify (lgl, default = TRUE) whether simplify output na_if_error (lgl, default = FALSE) whether return NA error occurs res_name (chr, default = \"gpt_res\") name column containing result .progress (lgl, default = TRUE) whether show progress bar seed (chr, default = NULL) string seed random number use_py (lgl, default = FALSE) whether use python ","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt_on_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"(tibble) result query","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/query_gpt_on_column.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query GPT on a dataframe's column — query_gpt_on_column","text":"","code":"if (FALSE) {   db <- tibble(    commenti = c(      \"deadly boring!\",      \"A bit boring, but interesting\",      \"How nice, I loved it!\"    )  )   role <- \"Sei l'assistente di un docente universitario.\"  context <- \"State analizzando i commenti degli studenti dell'ultimo corso.\"  task <- \"Il tuo compito è capire se sono soddisfatti del corso.\"  instructions <- \"Analizza i commenti e decidi se sono soddisfatti o meno.\"  output <- \"Riporta 'soddisfatto' o 'insoddisfatto'.\"  style <- \"Non aggiungere nessun commento, restituisci solo ed    esclusivamente la classificazione.\"  examples <- \"  commento_1: 'Mi è piaciuto molto il corso; davvero interessante.'  classificazione_1: 'soddisfatto'  commento_2: 'Non mi è piaciuto per niente; una noia mortale'  classificazione_2: 'insoddisfatto'  \"   sys_prompt <- compose_sys_prompt(role = role, context = context)  usr_prompt <- compose_usr_prompt(    task = task, instructions = instructions, output = output,    style = style, examples = examples  )  res <- db |>   query_gpt_on_column(     \"commenti\", sys_prompt = sys_prompt, usr_prompt = usr_prompt   )  res }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/setup_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup Python environment — setup_py","title":"Setup Python environment — setup_py","text":"function creates virtual environment installs openai package .","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/setup_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup Python environment — setup_py","text":"","code":"setup_py(venv_name = \"r-gpt-venv\", ask = interactive())"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/setup_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup Python environment — setup_py","text":"venv_name (chr, default \"r-gpt-venv\") name virtual environment created. ask (lgl, default TRUE interactive session, FALSE otherwise) TRUE, user asked want create virtual environment.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/setup_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup Python environment — setup_py","text":"(lgl) TRUE virtual environment created, FALSE otherwise.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/setup_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup Python environment — setup_py","text":"","code":"if (FALSE) {   library(gpteasyr)   setup_py()    prompt <- compose_prompt_api(     sys_prompt = \"You are the assistant of a university professor.\",     usr_prompt = \"Tell me about the last course you provided.\"   )    res <- query_gpt(       prompt = prompt,       use_py = TRUE     ) |>       get_content()     cat(res) }"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/write_jsonl_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Write jsonl files — write_jsonl_files","title":"Write jsonl files — write_jsonl_files","text":"function writes jsonl files list jsonl records.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/write_jsonl_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write jsonl files — write_jsonl_files","text":"","code":"write_jsonl_files(   jsonl_records,   dir_path,   name_prefix = stringr::str_c(stringr::str_remove_all(Sys.time(), \"\\\\D\"),     \"_batch-input\"),   max_mb = 100 )"},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/write_jsonl_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write jsonl files — write_jsonl_files","text":"jsonl_records (list) list jsonl records. dir_path (chr) directory path write jsonl files. name_prefix (chr, default = \"batch-input\") prefix jsonl files. max_mb (numeric, default = 100) maximum size jsonl files MB.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/write_jsonl_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write jsonl files — write_jsonl_files","text":"(invisible) jsonl records.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/reference/write_jsonl_files.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write jsonl files — write_jsonl_files","text":"","code":"role <- \"Sei l'assistente di un docente universitario.\"   context <- \"State analizzando i commenti degli studenti dell'ultimo corso.\"   task <- \"Il tuo compito è capire se sono soddisfatti del corso.\"   instructions <- \"Analizza i commenti e decidi se sono soddisfatti o meno.\"   output <- \"Riporta 'soddisfatto' o 'insoddisfatto'.\"   style <- \"Non aggiungere nessun commento, restituisci solo ed      esclusivamente la classificazione.\"   examples <- \"     commento_1: 'Mi è piaciuto molto il corso; davvero interessante.'     classificazione_1: 'soddisfatto'     commento_2: 'Non mi è piaciuto per niente; una noia mortale'     classificazione_2: 'insoddisfatto'     \"    sys_prompt <- compose_sys_prompt(role = role, context = context)   usr_prompt <- compose_usr_prompt(     task = task, instructions = instructions, output = output,     style = style, examples = examples   )    prompter <- create_usr_data_prompter(usr_prompt = usr_prompt)    jsonl_on_db <- tibble::tibble(     commenti = c(       \"deadly boring!\",       \"A bit boring, but interesting\",       \"How nice, I loved it!\"     )   ) |>     dplyr::mutate(       id = dplyr::row_number(),       prompt = commenti |>         purrr::map(           \\(x) compose_prompt_api(             sys_prompt = sys_prompt,             usr_prompt = prompter(x)           )         ),       jsonl = create_jsonl_records(prompt, id)     ) |>     dplyr::pull(jsonl)    # eval   temp_dir <- tempdir()   write_jsonl_files(jsonl_on_db, temp_dir)"},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-050","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.5.0","title":"gpteasyr 0.5.0","text":"Switch gpt-3.5-turbo cheapest default gpt-4o-mini Updated README documentations fix issue can lead integer overflow write_jsonl_files. Added file_* family functions","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-040","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.4.0","title":"gpteasyr 0.4.0","text":"Added file_upload, batch_create, batch_status, batch_cancel, batch_list, batch_result functions manage batch queries accordingly OpenAi API specification. Added create_jsonl_records, write_jsonl_files functions compose ‘jsonl’ files useful batch queries accordingly API specification. Model’s options available like query_gpt, gpt_query_on_column, get_completion_from_messages.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-030","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.3.0","title":"gpteasyr 0.3.0","text":"Changed name repo ‘CorradoLanera/gpteasyr’.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-029","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.9","title":"gpteasyr 0.2.9","text":"Added setup_py function setup python backend. update README","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-028","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.8","title":"gpteasyr 0.2.8","text":"Added use_py argument gpt_query, gpt_query_on_column, get_completion_from_messages","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-027","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.7","title":"gpteasyr 0.2.7","text":"Added closing argument compose_usr_prompt, compose_prompt, create_usr_data_prompter functions, allow add text end prompt, .e., embedded text.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-025","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.5","title":"gpteasyr 0.2.5","text":"Added seed argument gpt_query, gpt_query_on_column, get_completion_from_messages functions.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-024","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.4","title":"gpteasyr 0.2.4","text":"Now create_usr_data_prompter can accept custom delimiter. Default delimiter changed four quotes (\"\"\"\") three quotes (\"\"\").","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-023","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.3","title":"gpteasyr 0.2.3","text":"stream = FALSE hard coded (moment) get_completion_from_messages. column name results gpt_query_on_columns now customizable. Now gpt_query_on_columns returns original tibble column added , including option add return result new tibble single column (add = FALSE). Add progress bar gpt_query_on_column functions.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-022","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.2","title":"gpteasyr 0.2.2","text":"hot-fix old calls match.arg(model) gpt_query gpt_query_on_column functions.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-021","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.1","title":"gpteasyr 0.2.1","text":"Add option return NA API returns error; apply gpt_query gpt_query_on_column functions (.e., base get_completion_from_messages function).","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-020","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.2.0","title":"gpteasyr 0.2.0","text":"Removed dependency openai favor httr jsonlite directly Now queries can made personalized endpoints. create_usr_data_prompter now works empty characters (treated NULL). Now compose_prompt_api correctly manage empty prompts.","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-011","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.1.1","title":"gpteasyr 0.1.1","text":"100% coverage passed Activated tests CI","code":""},{"path":"https://CorradoLanera.github.io/gpteasyr/news/index.html","id":"gpteasyr-010","dir":"Changelog","previous_headings":"","what":"gpteasyr 0.1.0","title":"gpteasyr 0.1.0","text":"functions tested. Prompt compositors (.e., compose_prompt, compose_usr_prompt, compose_sys_prompt, create_usr_data_prompter) now always return character vector (possibly length 0). query_gpt_on_column now accepts sys_prompt usr_prompt arguments customize user system prompts. (fix #1) Changed functions names uniform. .e., compose_prompt_user compose_prompt_system now called compose_usr_prompt compose_sys_prompt; usr_msg sys_msg arguments now called usr_prompt sys_prompt. (fix #2) Added zzz.R startup messages checking API keys. Update README examples usage. Setup development environment. Initial setup CorradoLanera/gpt-template.","code":""}]
